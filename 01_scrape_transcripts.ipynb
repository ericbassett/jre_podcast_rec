{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python Version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "# auto reload imports that change\n",
    "%load_ext autoreload\n",
    "# only set to auto reload for marked imports\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "%aimport utils.scrape\n",
    "from utils import scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls for JRE transcripts\n",
    "url_podgist = 'https://www.podgist.com/joe-rogan-experience/index.html'\n",
    "url_podscribe = 'https://podscribe.app/feeds/http-joeroganexpjoeroganlibsynprocom-rss'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in websites\n",
    "resp_podgist = requests.get(url_podgist)\n",
    "resp_podscribe = requests.get(url_podscribe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautiful Soup pages\n",
    "page_podgist = BeautifulSoup(resp_podgist.text, 'html.parser')\n",
    "page_podscribe = BeautifulSoup(resp_podscribe.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Podgist URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 805 urls\n"
     ]
    }
   ],
   "source": [
    "# list in podgist\n",
    "podgist_episode_list = []\n",
    "\n",
    "# domain\n",
    "uri_podgist = urlparse(url_podgist)\n",
    "domain_podgist = '{uri.scheme}://{uri.netloc}'.format(uri=uri_podgist)\n",
    "\n",
    "# get rows in table\n",
    "rows_podcasts = (\n",
    "    page_podgist\n",
    "    .find('main', role='source-list')\n",
    "    .find('table')\n",
    "    .findAll('tr')\n",
    ")\n",
    "\n",
    "# loop through rows looking for link\n",
    "for row in rows_podcasts:\n",
    "    # cells in row\n",
    "    cells = row.findAll('td')\n",
    "    \n",
    "    # look for 2 or greater cells\n",
    "    if len(cells) >= 2:\n",
    "        \n",
    "        # look for span with transcribed icon\n",
    "        if cells[0].find('span', class_='icon-ghost icon-transcribed'):\n",
    "            \n",
    "            # empty episode dict\n",
    "            episode = {}\n",
    "            \n",
    "            \n",
    "            # get episode name and url\n",
    "            episode['name'] = cells[1].find('a').text\n",
    "            episode['url'] = domain_podgist+cells[1].find('a')['href']\n",
    "            \n",
    "            # get number\n",
    "            match_episode_num = re.search('\\d+',episode['name'])\n",
    "            \n",
    "            # append number if you have it\n",
    "            if match_episode_num:\n",
    "                episode['number'] = match_episode_num.group(0)\n",
    "            else:\n",
    "                episode['number'] = None\n",
    "                \n",
    "            # append to list\n",
    "            podgist_episode_list.append(deepcopy(episode))\n",
    "\n",
    "print(f'Scraped {len(podgist_episode_list)} urls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Podscribe URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 322 urls\n"
     ]
    }
   ],
   "source": [
    "# list in podscribe\n",
    "podscribe_episode_list = []\n",
    "\n",
    "# domain\n",
    "uri_podscribe = urlparse(url_podscribe)\n",
    "domain_podscribe = '{uri.scheme}://{uri.netloc}'.format(uri=uri_podscribe)\n",
    "\n",
    "# get rows in table\n",
    "rows_podcasts = (\n",
    "    page_podscribe\n",
    "    .find('table')\n",
    "    .find('tbody')\n",
    "    .findAll('tr')\n",
    ")\n",
    "\n",
    "# loop through rows looking for link\n",
    "for row in rows_podcasts:\n",
    "    # cells in row\n",
    "    cells = row.findAll('td')\n",
    "    \n",
    "    # look for 2 or greater cells\n",
    "    if len(cells) >= 2:\n",
    "        \n",
    "        # look for episode link\n",
    "        episode_cell = cells[1].find('a')\n",
    "        \n",
    "        if episode_cell:\n",
    "            \n",
    "            # look for episode transcribed icon\n",
    "            if episode_cell.find('i'):\n",
    "                \n",
    "                # empty episode dict\n",
    "                episode = {}\n",
    "                \n",
    "                # get episode name and number\n",
    "                episode['name'] = episode_cell.text\n",
    "                episode['url'] = domain_podscribe+episode_cell['href']\n",
    "\n",
    "                # get number\n",
    "                match_episode_num = re.search('\\d+',episode['name'])\n",
    "                \n",
    "                if match_episode_num:\n",
    "                    episode['number'] = match_episode_num.group(0)\n",
    "                else:\n",
    "                    episode['number'] = None\n",
    "                \n",
    "                # append episode\n",
    "                podscribe_episode_list.append(deepcopy(episode))\n",
    "                \n",
    "print(f'Scraped {len(podscribe_episode_list)} urls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Podgist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log file\n",
    "prog_file = open('logs/podgist_progress.txt','a')\n",
    "err_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mongo documents\n",
    "mongo_docs = []\n",
    "\n",
    "# total podcasts\n",
    "tot = len(podgist_episode_list)\n",
    "\n",
    "# go through episodes\n",
    "for i, ep in enumerate(podgist_episode_list):\n",
    "    # get page\n",
    "    resp_ep = scrape.get_page(ep['url'], 2.0)\n",
    "    \n",
    "    # check status 200 (good)\n",
    "    if resp_ep.status_code != 200:\n",
    "        \n",
    "        if not err_file:\n",
    "            err_file = open('logs/podgist_errors.txt','a')\n",
    "        \n",
    "        # write error to log    \n",
    "        scrape.rec_error(err_file, f\"status code !=200 for url: {ep['url']}\")\n",
    "    \n",
    "    # soup\n",
    "    soup_ep = BeautifulSoup(resp_ep.text,'html.parser')\n",
    "    \n",
    "    # parse\n",
    "    text_ep = ''\n",
    "\n",
    "    try:\n",
    "        # get transcription divs\n",
    "        for div in soup_ep.findAll('div', class_='transcription'):\n",
    "            \n",
    "            # spans store transcript text\n",
    "            for span in div.findAll('span'):\n",
    "                text_ep += span.text\n",
    "        \n",
    "        # for mongo\n",
    "        mongo_ep = {}\n",
    "        mongo_ep.update(deepcopy(ep))\n",
    "        mongo_ep['text'] = deepcopy(text_ep)\n",
    "        mongo_ep['source'] = 'podgist'\n",
    "        mongo_docs.append(deepcopy(mongo_ep))\n",
    "        \n",
    "    except Exception as e:\n",
    "        if not err_file:\n",
    "            err_file = open('logs/podgist_errors.txt','a')\n",
    "        scrape.rec_error(err_file, f\"Error {str(e)}\\nurl: {ep['url']}\")\n",
    "        continue\n",
    "        \n",
    "    scrape.rec_progress(i, tot, prog_file, ep['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close files\n",
    "prog_file.close()\n",
    "if err_file:\n",
    "    err_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Podscribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log file\n",
    "prog_file = open('logs/podscribe_progress.txt','a')\n",
    "err_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total podcasts\n",
    "tot = len(podscribe_episode_list)\n",
    "\n",
    "# go through episodes\n",
    "for i, ep in enumerate(podscribe_episode_list):\n",
    "    # get page\n",
    "    resp_ep = scrape.get_page(ep['url'], 2.0)\n",
    "    \n",
    "    # check status 200 (good)\n",
    "    if resp_ep.status_code != 200:\n",
    "        \n",
    "        if not err_file:\n",
    "            err_file = open('logs/podscribe_errors.txt','a')\n",
    "        \n",
    "        # write error to log    \n",
    "        scrape.rec_error(err_file, f\"status code !=200 for url: {ep['url']}\")\n",
    "    \n",
    "    # soup\n",
    "    soup_ep = BeautifulSoup(resp_ep.text,'html.parser')\n",
    "    \n",
    "    # parse\n",
    "    text_ep = ''\n",
    "\n",
    "    try:\n",
    "        # episode description\n",
    "        ep_desc = (\n",
    "            soup_ep\n",
    "            .find('main')\n",
    "            .find('p', class_='episode-content')\n",
    "            .text\n",
    "        )\n",
    "\n",
    "        # get transcription p's\n",
    "        ep_p_list = (\n",
    "            soup_ep\n",
    "            .find('main')\n",
    "            .find('p', class_=None)\n",
    "            .findAllNext('p', class_='')\n",
    "        )\n",
    "\n",
    "        for p in ep_p_list:\n",
    "            text_ep += p.text\n",
    "\n",
    "        # for mongo\n",
    "        mongo_ep = {}\n",
    "        mongo_ep.update(deepcopy(ep))\n",
    "        mongo_ep['text'] = deepcopy(text_ep)\n",
    "        mongo_ep['source'] = 'podscribe'\n",
    "        mongo_ep['desc'] = ep_desc\n",
    "        mongo_docs.append(deepcopy(mongo_ep))\n",
    "\n",
    "    except Exception as e:\n",
    "        if not err_file:\n",
    "            err_file = open('logs/podscribe_errors.txt','a')\n",
    "        scrape.rec_error(err_file, f\"Error {str(e)}\\nurl: {ep['url']}\")\n",
    "        continue\n",
    "        \n",
    "    scrape.rec_progress(i, tot, prog_file, ep['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close files\n",
    "prog_file.close()\n",
    "if err_file:\n",
    "    err_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "%aimport credentials.cred\n",
    "from credentials import cred\n",
    "\n",
    "\n",
    "config = {\n",
    "  'host': cred.mongo_host,\n",
    "  'username': cred.mongo_user,\n",
    "  'password': cred.mongo_pass,\n",
    "  'authSource': cred.mongo_auth_db\n",
    "}\n",
    "\n",
    "# get a mongo client\n",
    "client = MongoClient(**config)\n",
    "\n",
    "# use the raw database\n",
    "jre_raw = client.jre_raw\n",
    "podcasts = jre_raw.podcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push to Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f6baff22960>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcasts.insert_many(mongo_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcasts.count_documents({}) == len(mongo_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
