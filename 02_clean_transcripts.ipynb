{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python Version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# auto reload imports that change\n",
    "%load_ext autoreload\n",
    "# only set to auto reload for marked imports\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "%aimport credentials.cred\n",
    "from credentials import cred\n",
    "\n",
    "\n",
    "config = {\n",
    "  'host': cred.mongo_host,\n",
    "  'username': cred.mongo_user,\n",
    "  'password': cred.mongo_pass,\n",
    "  'authSource': cred.mongo_auth_db\n",
    "}\n",
    "\n",
    "# get a mongo client\n",
    "client = MongoClient(**config)\n",
    "\n",
    "# use the raw database\n",
    "jre_raw = client.jre_raw\n",
    "podcasts_raw = jre_raw.podcasts\n",
    "\n",
    "# use the clean database\n",
    "jre_clean = client.jre_clean\n",
    "podcasts_clean = jre_clean.podcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Podcasts from Podscribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "ps_ep_mongo = list(\n",
    "    podcasts_raw\n",
    "    .find({'source':'podscribe'},{'_id':0, 'text':1, 'name':1})\n",
    "    .limit(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out text and names\n",
    "ps_ep_list = [x['text'] for x in ps_ep_mongo]\n",
    "ps_ep_names = [x['name'] for x in ps_ep_mongo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_ep_list = [x.lower() for x in ps_ep_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "new_stop_words = []\n",
    "\n",
    "# profanity\n",
    "profanity = (\n",
    "    open('stop_words/profanity.txt', newline='\\n')\n",
    "    .read()\n",
    "    .splitlines()\n",
    ")\n",
    "\n",
    "new_stop_words += [x.lower() for x in profanity]\n",
    "\n",
    "# common Joe Rogan words\n",
    "common_jre_words = [\n",
    "    'like', 'yeah', 'know', 'just', 'right', 'right', 'think', 'know',\n",
    "    'people', 'going', 'really', 'got', 'thing', 'want', 'actually',\n",
    "    'say'\n",
    "]\n",
    "new_stop_words += common_jre_words\n",
    "\n",
    "# append to english stop words\n",
    "jre_stop_words = text.ENGLISH_STOP_WORDS.union(new_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i, ep in enumerate(ps_ep_list):\n",
    "    ps_ep_list[i] = ' '.join([lemmatizer.lemmatize(x) for x in ep.split(' ')])\n",
    "    \n",
    "jre_stop_words = [lemmatizer.lemmatize(x) for x in jre_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
       "                min_df=0.02, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True,\n",
       "                stop_words=['assfukka', 'toward', 'very', 'fill', 'them',\n",
       "                            'hoer', 'ejaculated', 'all', 'bastard', 'wanker',\n",
       "                            'please', 'dogging', 'have', 'fanny', 'meanwhile',\n",
       "                            'latterly', 'during', 'either', 'shitfull',\n",
       "                            'twatty', 'moreover', 'everywhere', 'dink',\n",
       "                            'breast', 'of', 'pisser', 'per', 'fuckwhit', 'been',\n",
       "                            'fuckhead', ...],\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='\\\\b[a-z][a-z]+\\\\b', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "ps_ep_list_tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2), binary=True, stop_words=jre_stop_words,\n",
    "    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", max_df=0.5, min_df=0.02\n",
    ")\n",
    "\n",
    "# fit\n",
    "ps_ep_list_tfidf.fit(ps_ep_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.5, max_features=None, min_df=0.02,\n",
       "                ngram_range=(1, 3), preprocessor=None,\n",
       "                stop_words=['assfukka', 'toward', 'very', 'fill', 'them',\n",
       "                            'hoer', 'ejaculated', 'all', 'bastard', 'wanker',\n",
       "                            'please', 'dogging', 'have', 'fanny', 'meanwhile',\n",
       "                            'latterly', 'during', 'either', 'shitfull',\n",
       "                            'twatty', 'moreover', 'everywhere', 'dink',\n",
       "                            'breast', 'of', 'pisser', 'per', 'fuckwhit', 'been',\n",
       "                            'fuckhead', ...],\n",
       "                strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer for parsing/counting words\n",
    "ps_ep_list_cvec = CountVectorizer(\n",
    "    ngram_range=(1, 3), stop_words=jre_stop_words,\n",
    "    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",  max_df=0.5, min_df=0.02\n",
    ")\n",
    "\n",
    "# fit\n",
    "ps_ep_list_cvec.fit(ps_ep_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_ep_list_tokenizer = ps_ep_list_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# column names\n",
    "col_names = ['component_'+str(x) for x in range(1,num_topics+1)]\n",
    "\n",
    "# run LDA?\n",
    "run_lda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "if run_lda:\n",
    "    doc_word = ps_ep_list_tokenizer.transform(ps_ep_list).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "if run_lda:\n",
    "    corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lda:    \n",
    "    id2word = dict((v, k) for k, v in ps_ep_list_tokenizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 20:03:20,751 : INFO : using symmetric alpha at 0.2\n",
      "2020-02-19 20:03:20,753 : INFO : using symmetric eta at 0.2\n",
      "2020-02-19 20:03:20,803 : INFO : using serial LDA version on this node\n",
      "2020-02-19 20:03:20,880 : INFO : running online (multi-pass) LDA training, 5 topics, 50 passes over the supplied corpus of 100 documents, updating model once every 100 documents, evaluating perplexity every 100 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-02-19 20:03:24,843 : INFO : -67.602 per-word bound, 224038040386537127936.0 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:03:24,845 : INFO : PROGRESS: pass 0, at document #100/100\n",
      "2020-02-19 20:03:25,679 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"truly\" + 0.000*\"stress\" + 0.000*\"communicate\" + 0.000*\"did guy\" + 0.000*\"paying attention\" + 0.000*\"medical\" + 0.000*\"don hear\" + 0.000*\"metal\" + 0.000*\"used used\"\n",
      "2020-02-19 20:03:25,683 : INFO : topic #1 (0.200): 0.000*\"footage\" + 0.000*\"fought\" + 0.000*\"app cash\" + 0.000*\"jiu jitsu\" + 0.000*\"element\" + 0.000*\"time did\" + 0.000*\"legitimate\" + 0.000*\"station\" + 0.000*\"landed\" + 0.000*\"scare\"\n",
      "2020-02-19 20:03:25,686 : INFO : topic #2 (0.200): 0.000*\"specie\" + 0.000*\"let talk\" + 0.000*\"flow\" + 0.000*\"ren fight\" + 0.000*\"prison\" + 0.000*\"everybody thank\" + 0.000*\"experienced\" + 0.000*\"responsibility\" + 0.000*\"century\" + 0.000*\"hotel\"\n",
      "2020-02-19 20:03:25,688 : INFO : topic #3 (0.200): 0.000*\"app second\" + 0.000*\"ate\" + 0.000*\"investable\" + 0.000*\"critical\" + 0.000*\"dry\" + 0.000*\"experience train\" + 0.000*\"man mean\" + 0.000*\"studio\" + 0.000*\"turning\" + 0.000*\"support good\"\n",
      "2020-02-19 20:03:25,692 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"justin ren\" + 0.000*\"shoe\" + 0.000*\"approach\" + 0.000*\"day bank\" + 0.000*\"opened\" + 0.000*\"protected\" + 0.000*\"chinese\" + 0.000*\"little kid\" + 0.000*\"terrifying\"\n",
      "2020-02-19 20:03:25,693 : INFO : topic diff=2.970841, rho=1.000000\n",
      "2020-02-19 20:03:29,298 : INFO : -15.140 per-word bound, 36109.4 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:03:29,299 : INFO : PROGRESS: pass 1, at document #100/100\n",
      "2020-02-19 20:03:30,192 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"metal\" + 0.000*\"chart\"\n",
      "2020-02-19 20:03:30,195 : INFO : topic #1 (0.200): 0.000*\"factory\" + 0.000*\"scare\" + 0.000*\"problem problem\" + 0.000*\"jumped\" + 0.000*\"fought\" + 0.000*\"legitimate\" + 0.000*\"address\" + 0.000*\"app cash\" + 0.000*\"station\" + 0.000*\"footage\"\n",
      "2020-02-19 20:03:30,200 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"electric\" + 0.000*\"november\"\n",
      "2020-02-19 20:03:30,204 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"don come\" + 0.000*\"man mean\" + 0.000*\"experience train\" + 0.000*\"ate\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"balance\"\n",
      "2020-02-19 20:03:30,209 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"justin ren\" + 0.000*\"counter\" + 0.000*\"shoe\" + 0.000*\"approach\" + 0.000*\"zone\" + 0.000*\"little kid\" + 0.000*\"thousand year\" + 0.000*\"day bank\"\n",
      "2020-02-19 20:03:30,211 : INFO : topic diff=0.083918, rho=0.577350\n",
      "2020-02-19 20:03:33,612 : INFO : -14.989 per-word bound, 32510.8 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:03:33,613 : INFO : PROGRESS: pass 2, at document #100/100\n",
      "2020-02-19 20:03:34,319 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"metal\" + 0.000*\"chart\"\n",
      "2020-02-19 20:03:34,322 : INFO : topic #1 (0.200): 0.000*\"legitimate\" + 0.000*\"result\" + 0.000*\"factory\" + 0.000*\"station\" + 0.000*\"report\" + 0.000*\"university\" + 0.000*\"wheel\" + 0.000*\"scare\" + 0.000*\"function\" + 0.000*\"current\"\n",
      "2020-02-19 20:03:34,325 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"electric\" + 0.000*\"november\"\n",
      "2020-02-19 20:03:34,329 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:03:34,334 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:03:34,335 : INFO : topic diff=0.089635, rho=0.500000\n",
      "2020-02-19 20:03:37,576 : INFO : -14.769 per-word bound, 27920.4 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:03:37,577 : INFO : PROGRESS: pass 3, at document #100/100\n",
      "2020-02-19 20:03:38,080 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"metal\" + 0.000*\"chart\"\n",
      "2020-02-19 20:03:38,084 : INFO : topic #1 (0.200): 0.000*\"legitimate\" + 0.000*\"result\" + 0.000*\"station\" + 0.000*\"alien\" + 0.000*\"function\" + 0.000*\"report\" + 0.000*\"truly\" + 0.000*\"current\" + 0.000*\"method\" + 0.000*\"application\"\n",
      "2020-02-19 20:03:38,088 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"electric\" + 0.000*\"november\"\n",
      "2020-02-19 20:03:38,090 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:03:38,095 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:03:38,096 : INFO : topic diff=0.056864, rho=0.447214\n",
      "2020-02-19 20:03:41,171 : INFO : -14.665 per-word bound, 25979.7 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:03:41,172 : INFO : PROGRESS: pass 4, at document #100/100\n",
      "2020-02-19 20:03:41,657 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"metal\" + 0.000*\"chart\"\n",
      "2020-02-19 20:03:41,659 : INFO : topic #1 (0.200): 0.000*\"legitimate\" + 0.000*\"truly\" + 0.000*\"station\" + 0.000*\"result\" + 0.000*\"application\" + 0.000*\"alien\" + 0.000*\"function\" + 0.000*\"report\" + 0.000*\"current\" + 0.000*\"method\"\n",
      "2020-02-19 20:03:41,664 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"electric\" + 0.000*\"november\"\n",
      "2020-02-19 20:03:41,667 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:03:41,671 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:03:41,675 : INFO : topic diff=0.029775, rho=0.408248\n",
      "2020-02-19 20:03:44,729 : INFO : -14.629 per-word bound, 25337.0 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:03:44,730 : INFO : PROGRESS: pass 5, at document #100/100\n",
      "2020-02-19 20:03:45,223 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"metal\" + 0.000*\"chart\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 20:03:45,225 : INFO : topic #1 (0.200): 0.000*\"application\" + 0.000*\"truly\" + 0.000*\"legitimate\" + 0.000*\"train day\" + 0.000*\"alien\" + 0.000*\"station\" + 0.000*\"result\" + 0.000*\"approach\" + 0.000*\"function\" + 0.000*\"invest\"\n",
      "2020-02-19 20:03:45,229 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"electric\" + 0.000*\"november\"\n",
      "2020-02-19 20:03:45,232 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:03:45,236 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:03:45,238 : INFO : topic diff=0.016785, rho=0.377964\n",
      "2020-02-19 20:03:48,288 : INFO : -14.614 per-word bound, 25076.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:03:48,288 : INFO : PROGRESS: pass 6, at document #100/100\n",
      "2020-02-19 20:03:48,744 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"metal\" + 0.000*\"chart\"\n",
      "2020-02-19 20:03:48,748 : INFO : topic #1 (0.200): 0.000*\"application\" + 0.000*\"train day\" + 0.000*\"truly\" + 0.000*\"approach\" + 0.000*\"alien\" + 0.000*\"invest\" + 0.000*\"legitimate\" + 0.000*\"station\" + 0.000*\"experience train\" + 0.000*\"result\"\n",
      "2020-02-19 20:03:48,752 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"electric\" + 0.000*\"november\"\n",
      "2020-02-19 20:03:48,755 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:03:48,758 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:03:48,760 : INFO : topic diff=0.009880, rho=0.353553\n",
      "2020-02-19 20:03:51,817 : INFO : -14.607 per-word bound, 24954.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:03:51,817 : INFO : PROGRESS: pass 7, at document #100/100\n",
      "2020-02-19 20:03:52,266 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:03:52,268 : INFO : topic #1 (0.200): 0.000*\"application\" + 0.000*\"train day\" + 0.000*\"truly\" + 0.000*\"approach\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"alien\" + 0.000*\"station\" + 0.000*\"summer\" + 0.000*\"everybody thank\"\n",
      "2020-02-19 20:03:52,272 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"electric\" + 0.000*\"november\"\n",
      "2020-02-19 20:03:52,275 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:03:52,282 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:03:52,283 : INFO : topic diff=0.006072, rho=0.333333\n",
      "2020-02-19 20:03:55,348 : INFO : -14.603 per-word bound, 24890.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:03:55,349 : INFO : PROGRESS: pass 8, at document #100/100\n",
      "2020-02-19 20:03:55,809 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:03:55,811 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"truly\" + 0.000*\"approach\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"alien\" + 0.000*\"building pygmy\" + 0.000*\"summer\" + 0.000*\"everybody thank\"\n",
      "2020-02-19 20:03:55,815 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"november\" + 0.000*\"electric\"\n",
      "2020-02-19 20:03:55,819 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:03:55,822 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:03:55,824 : INFO : topic diff=0.003890, rho=0.316228\n",
      "2020-02-19 20:03:58,869 : INFO : -14.601 per-word bound, 24853.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:03:58,870 : INFO : PROGRESS: pass 9, at document #100/100\n",
      "2020-02-19 20:03:59,336 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:03:59,341 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"summer\" + 0.000*\"charity building\" + 0.000*\"alien\"\n",
      "2020-02-19 20:03:59,344 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"november\" + 0.000*\"electric\"\n",
      "2020-02-19 20:03:59,347 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:03:59,351 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:03:59,353 : INFO : topic diff=0.002560, rho=0.301511\n",
      "2020-02-19 20:04:02,407 : INFO : -14.600 per-word bound, 24830.9 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:02,408 : INFO : PROGRESS: pass 10, at document #100/100\n",
      "2020-02-19 20:04:02,856 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:02,860 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:02,864 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"november\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:02,867 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 20:04:02,872 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:02,874 : INFO : topic diff=0.001761, rho=0.288675\n",
      "2020-02-19 20:04:05,916 : INFO : -14.599 per-word bound, 24816.3 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:05,917 : INFO : PROGRESS: pass 11, at document #100/100\n",
      "2020-02-19 20:04:06,370 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:06,373 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:06,376 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"november\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:06,380 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:06,383 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:06,384 : INFO : topic diff=0.001236, rho=0.277350\n",
      "2020-02-19 20:04:09,423 : INFO : -14.598 per-word bound, 24806.6 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:09,423 : INFO : PROGRESS: pass 12, at document #100/100\n",
      "2020-02-19 20:04:09,894 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:09,896 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:09,900 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"november\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:09,903 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:09,907 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:09,909 : INFO : topic diff=0.000876, rho=0.267261\n",
      "2020-02-19 20:04:12,938 : INFO : -14.598 per-word bound, 24799.9 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:12,938 : INFO : PROGRESS: pass 13, at document #100/100\n",
      "2020-02-19 20:04:13,393 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:13,397 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:13,399 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"november\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:13,403 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:13,408 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:13,409 : INFO : topic diff=0.000634, rho=0.258199\n",
      "2020-02-19 20:04:16,450 : INFO : -14.598 per-word bound, 24795.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:16,451 : INFO : PROGRESS: pass 14, at document #100/100\n",
      "2020-02-19 20:04:16,905 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:16,907 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:16,912 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"november\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:16,916 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:16,919 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:16,921 : INFO : topic diff=0.000473, rho=0.250000\n",
      "2020-02-19 20:04:19,966 : INFO : -14.598 per-word bound, 24792.0 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:19,967 : INFO : PROGRESS: pass 15, at document #100/100\n",
      "2020-02-19 20:04:20,417 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:20,420 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:20,423 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"november\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:20,428 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:20,431 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:20,433 : INFO : topic diff=0.000360, rho=0.242536\n",
      "2020-02-19 20:04:23,460 : INFO : -14.597 per-word bound, 24788.9 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:23,461 : INFO : PROGRESS: pass 16, at document #100/100\n",
      "2020-02-19 20:04:23,913 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 20:04:23,915 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:23,919 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"november\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:23,922 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:23,926 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:23,928 : INFO : topic diff=0.000272, rho=0.235702\n",
      "2020-02-19 20:04:26,956 : INFO : -14.597 per-word bound, 24787.1 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:26,957 : INFO : PROGRESS: pass 17, at document #100/100\n",
      "2020-02-19 20:04:27,421 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:27,425 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:27,428 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"november\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:27,431 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:27,434 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:27,436 : INFO : topic diff=0.000208, rho=0.229416\n",
      "2020-02-19 20:04:30,486 : INFO : -14.597 per-word bound, 24785.5 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:30,487 : INFO : PROGRESS: pass 18, at document #100/100\n",
      "2020-02-19 20:04:30,960 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:30,962 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:30,968 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"november\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:30,976 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:30,980 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:30,981 : INFO : topic diff=0.000159, rho=0.223607\n",
      "2020-02-19 20:04:34,049 : INFO : -14.597 per-word bound, 24784.5 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:34,050 : INFO : PROGRESS: pass 19, at document #100/100\n",
      "2020-02-19 20:04:34,504 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:34,507 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:34,510 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"stuff ve\" + 0.000*\"bye everybody\" + 0.000*\"november\" + 0.000*\"guard\" + 0.000*\"pleasure\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:34,514 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:34,517 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:34,518 : INFO : topic diff=0.000126, rho=0.218218\n",
      "2020-02-19 20:04:37,558 : INFO : -14.597 per-word bound, 24783.5 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:37,559 : INFO : PROGRESS: pass 20, at document #100/100\n",
      "2020-02-19 20:04:38,013 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"paying attention\" + 0.000*\"chart\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:38,015 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:38,018 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"bye everybody\" + 0.000*\"stuff ve\" + 0.000*\"november\" + 0.000*\"pleasure\" + 0.000*\"guard\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:38,023 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:38,028 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:38,029 : INFO : topic diff=0.000097, rho=0.213201\n",
      "2020-02-19 20:04:41,063 : INFO : -14.597 per-word bound, 24783.1 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:41,064 : INFO : PROGRESS: pass 21, at document #100/100\n",
      "2020-02-19 20:04:41,538 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"chart\" + 0.000*\"paying attention\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:41,540 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:41,543 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"stuff ve\" + 0.000*\"bye everybody\" + 0.000*\"november\" + 0.000*\"pleasure\" + 0.000*\"guard\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:41,549 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"nutrient\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 20:04:41,551 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:41,553 : INFO : topic diff=0.000075, rho=0.208514\n",
      "2020-02-19 20:04:44,604 : INFO : -14.597 per-word bound, 24782.3 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:44,604 : INFO : PROGRESS: pass 22, at document #100/100\n",
      "2020-02-19 20:04:45,058 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"chart\" + 0.000*\"paying attention\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:45,060 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:45,064 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"stuff ve\" + 0.000*\"november\" + 0.000*\"bye everybody\" + 0.000*\"guard\" + 0.000*\"electric\" + 0.000*\"pleasure\"\n",
      "2020-02-19 20:04:45,067 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"nutrient\" + 0.000*\"doe make\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:45,070 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:45,072 : INFO : topic diff=0.000060, rho=0.204124\n",
      "2020-02-19 20:04:48,170 : INFO : -14.597 per-word bound, 24782.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:48,171 : INFO : PROGRESS: pass 23, at document #100/100\n",
      "2020-02-19 20:04:48,623 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"chart\" + 0.000*\"paying attention\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:48,625 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:48,628 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"november\" + 0.000*\"stuff ve\" + 0.000*\"bye everybody\" + 0.000*\"pleasure\" + 0.000*\"guard\" + 0.000*\"electric\"\n",
      "2020-02-19 20:04:48,631 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"nutrient\" + 0.000*\"doe make\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:48,634 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:48,636 : INFO : topic diff=0.000048, rho=0.200000\n",
      "2020-02-19 20:04:51,677 : INFO : -14.597 per-word bound, 24781.8 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:51,678 : INFO : PROGRESS: pass 24, at document #100/100\n",
      "2020-02-19 20:04:52,125 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"chart\" + 0.000*\"paying attention\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:52,128 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:52,132 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"november\" + 0.000*\"stuff ve\" + 0.000*\"bye everybody\" + 0.000*\"electric\" + 0.000*\"pleasure\" + 0.000*\"guard\"\n",
      "2020-02-19 20:04:52,135 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"nutrient\" + 0.000*\"doe make\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:52,138 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:52,140 : INFO : topic diff=0.000038, rho=0.196116\n",
      "2020-02-19 20:04:55,190 : INFO : -14.597 per-word bound, 24781.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:55,190 : INFO : PROGRESS: pass 25, at document #100/100\n",
      "2020-02-19 20:04:55,644 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"chart\" + 0.000*\"paying attention\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:55,646 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:55,650 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"november\" + 0.000*\"stuff ve\" + 0.000*\"bye everybody\" + 0.000*\"pleasure\" + 0.000*\"electric\" + 0.000*\"guard\"\n",
      "2020-02-19 20:04:55,653 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"nutrient\" + 0.000*\"doe make\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:55,657 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:55,658 : INFO : topic diff=0.000030, rho=0.192450\n",
      "2020-02-19 20:04:58,694 : INFO : -14.597 per-word bound, 24781.1 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:04:58,694 : INFO : PROGRESS: pass 26, at document #100/100\n",
      "2020-02-19 20:04:59,149 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"method\" + 0.000*\"free shipping\" + 0.000*\"chart\" + 0.000*\"paying attention\" + 0.000*\"metal\"\n",
      "2020-02-19 20:04:59,152 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:04:59,155 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"november\" + 0.000*\"stuff ve\" + 0.000*\"bye everybody\" + 0.000*\"electric\" + 0.000*\"pleasure\" + 0.000*\"guard\"\n",
      "2020-02-19 20:04:59,158 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"nutrient\" + 0.000*\"doe make\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:04:59,161 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"surprised\"\n",
      "2020-02-19 20:04:59,164 : INFO : topic diff=0.000024, rho=0.188982\n",
      "2020-02-19 20:05:02,218 : INFO : -14.597 per-word bound, 24781.0 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:02,219 : INFO : PROGRESS: pass 27, at document #100/100\n",
      "2020-02-19 20:05:02,675 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"free shipping\" + 0.000*\"chart\" + 0.000*\"method\" + 0.000*\"paying attention\" + 0.000*\"metal\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 20:05:02,678 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:02,682 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"november\" + 0.000*\"stuff ve\" + 0.000*\"bye everybody\" + 0.000*\"electric\" + 0.000*\"pleasure\" + 0.000*\"guard\"\n",
      "2020-02-19 20:05:02,687 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"nutrient\" + 0.000*\"doe make\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:05:02,690 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"lot different\" + 0.000*\"aunt\"\n",
      "2020-02-19 20:05:02,691 : INFO : topic diff=0.000020, rho=0.185695\n",
      "2020-02-19 20:05:05,829 : INFO : -14.597 per-word bound, 24780.7 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:05,830 : INFO : PROGRESS: pass 28, at document #100/100\n",
      "2020-02-19 20:05:06,285 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"chart\" + 0.000*\"free shipping\" + 0.000*\"method\" + 0.000*\"metal\" + 0.000*\"paying attention\"\n",
      "2020-02-19 20:05:06,287 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:06,290 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"november\" + 0.000*\"stuff ve\" + 0.000*\"bye everybody\" + 0.000*\"electric\" + 0.000*\"pleasure\" + 0.000*\"guard\"\n",
      "2020-02-19 20:05:06,294 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"nutrient\" + 0.000*\"doe make\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:05:06,296 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"justin ren\" + 0.000*\"explained\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"tie\" + 0.000*\"pasta\" + 0.000*\"lot different\"\n",
      "2020-02-19 20:05:06,299 : INFO : topic diff=0.000016, rho=0.182574\n",
      "2020-02-19 20:05:09,366 : INFO : -14.597 per-word bound, 24780.6 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:09,366 : INFO : PROGRESS: pass 29, at document #100/100\n",
      "2020-02-19 20:05:09,819 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"stress\" + 0.000*\"used used\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"chart\" + 0.000*\"free shipping\" + 0.000*\"method\" + 0.000*\"paying attention\" + 0.000*\"liberal\"\n",
      "2020-02-19 20:05:09,822 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:09,827 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"november\" + 0.000*\"stuff ve\" + 0.000*\"bye everybody\" + 0.000*\"electric\" + 0.000*\"pleasure\" + 0.000*\"guard\"\n",
      "2020-02-19 20:05:09,830 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"nutrient\" + 0.000*\"doe make\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"dry\" + 0.000*\"come home\"\n",
      "2020-02-19 20:05:09,833 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"explained\" + 0.000*\"justin ren\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"pasta\" + 0.000*\"tie\" + 0.000*\"aunt\"\n",
      "2020-02-19 20:05:09,835 : INFO : topic diff=0.000013, rho=0.179605\n",
      "2020-02-19 20:05:12,898 : INFO : -14.597 per-word bound, 24780.5 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:12,899 : INFO : PROGRESS: pass 30, at document #100/100\n",
      "2020-02-19 20:05:13,352 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"used used\" + 0.000*\"stress\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"chart\" + 0.000*\"free shipping\" + 0.000*\"method\" + 0.000*\"metal\" + 0.000*\"paying attention\"\n",
      "2020-02-19 20:05:13,354 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:13,358 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"november\" + 0.000*\"stuff ve\" + 0.000*\"electric\" + 0.000*\"bye everybody\" + 0.000*\"guard\" + 0.000*\"pleasure\"\n",
      "2020-02-19 20:05:13,361 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"nutrient\" + 0.000*\"doe make\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"come home\" + 0.000*\"dry\"\n",
      "2020-02-19 20:05:13,364 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"explained\" + 0.000*\"justin ren\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"pasta\" + 0.000*\"aunt\" + 0.000*\"tie\"\n",
      "2020-02-19 20:05:13,366 : INFO : topic diff=0.000010, rho=0.176777\n",
      "2020-02-19 20:05:16,443 : INFO : -14.597 per-word bound, 24780.5 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:16,444 : INFO : PROGRESS: pass 31, at document #100/100\n",
      "2020-02-19 20:05:16,897 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"used used\" + 0.000*\"stress\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"chart\" + 0.000*\"free shipping\" + 0.000*\"providing\" + 0.000*\"method\" + 0.000*\"liberal\"\n",
      "2020-02-19 20:05:16,899 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:16,902 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"november\" + 0.000*\"prison\" + 0.000*\"stuff ve\" + 0.000*\"electric\" + 0.000*\"bye everybody\" + 0.000*\"emotional\" + 0.000*\"guard\"\n",
      "2020-02-19 20:05:16,906 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"nutrient\" + 0.000*\"doe make\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"come home\" + 0.000*\"dry\"\n",
      "2020-02-19 20:05:16,908 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"explained\" + 0.000*\"justin ren\" + 0.000*\"teaching\" + 0.000*\"pasta\" + 0.000*\"day bank\" + 0.000*\"aunt\" + 0.000*\"nate\"\n",
      "2020-02-19 20:05:16,911 : INFO : topic diff=0.000009, rho=0.174078\n",
      "2020-02-19 20:05:19,953 : INFO : -14.597 per-word bound, 24780.4 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:19,953 : INFO : PROGRESS: pass 32, at document #100/100\n",
      "2020-02-19 20:05:20,404 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"used used\" + 0.000*\"stress\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"chart\" + 0.000*\"providing\" + 0.000*\"free shipping\" + 0.000*\"method\" + 0.000*\"liberal\"\n",
      "2020-02-19 20:05:20,408 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:20,412 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"november\" + 0.000*\"prison\" + 0.000*\"stuff ve\" + 0.000*\"electric\" + 0.000*\"bye everybody\" + 0.000*\"pleasure\" + 0.000*\"emotional\"\n",
      "2020-02-19 20:05:20,415 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"nutrient\" + 0.000*\"doe make\" + 0.000*\"don try\" + 0.000*\"bank transfer\" + 0.000*\"come home\" + 0.000*\"dry\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 20:05:20,420 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"explained\" + 0.000*\"justin ren\" + 0.000*\"pasta\" + 0.000*\"teaching\" + 0.000*\"day bank\" + 0.000*\"nate\" + 0.000*\"versa\"\n",
      "2020-02-19 20:05:20,422 : INFO : topic diff=0.000007, rho=0.171499\n",
      "2020-02-19 20:05:23,482 : INFO : -14.597 per-word bound, 24780.1 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:23,483 : INFO : PROGRESS: pass 33, at document #100/100\n",
      "2020-02-19 20:05:23,935 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"used used\" + 0.000*\"stress\" + 0.000*\"communicate\" + 0.000*\"truly\" + 0.000*\"chart\" + 0.000*\"providing\" + 0.000*\"free shipping\" + 0.000*\"method\" + 0.000*\"liberal\"\n",
      "2020-02-19 20:05:23,938 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:23,941 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"november\" + 0.000*\"prison\" + 0.000*\"stuff ve\" + 0.000*\"madness\" + 0.000*\"electric\" + 0.000*\"month month\" + 0.000*\"pleasure\"\n",
      "2020-02-19 20:05:23,945 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"nutrient\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"come home\" + 0.000*\"let don\"\n",
      "2020-02-19 20:05:23,948 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"pasta\" + 0.000*\"explained\" + 0.000*\"justin ren\" + 0.000*\"versa\" + 0.000*\"nate\" + 0.000*\"teaching\" + 0.000*\"course didn\"\n",
      "2020-02-19 20:05:23,950 : INFO : topic diff=0.000006, rho=0.169031\n",
      "2020-02-19 20:05:26,993 : INFO : -14.597 per-word bound, 24780.3 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:26,994 : INFO : PROGRESS: pass 34, at document #100/100\n",
      "2020-02-19 20:05:27,445 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"used used\" + 0.000*\"stress\" + 0.000*\"communicate\" + 0.000*\"chart\" + 0.000*\"truly\" + 0.000*\"providing\" + 0.000*\"cbd oil\" + 0.000*\"liberal\" + 0.000*\"free shipping\"\n",
      "2020-02-19 20:05:27,449 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:27,453 : INFO : topic #2 (0.200): 0.000*\"everybody thank\" + 0.000*\"november\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"prison\" + 0.000*\"ve launched\" + 0.000*\"stuff ve\" + 0.000*\"madness\" + 0.000*\"month month\" + 0.000*\"electric\"\n",
      "2020-02-19 20:05:27,456 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"nutrient\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"come home\" + 0.000*\"bank transfer\" + 0.000*\"don try\" + 0.000*\"let don\"\n",
      "2020-02-19 20:05:27,458 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"pasta\" + 0.000*\"versa\" + 0.000*\"course didn\" + 0.000*\"nate\" + 0.000*\"explained\" + 0.000*\"justin ren\" + 0.000*\"teaching\"\n",
      "2020-02-19 20:05:27,460 : INFO : topic diff=0.000005, rho=0.166667\n",
      "2020-02-19 20:05:30,524 : INFO : -14.597 per-word bound, 24780.4 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:30,525 : INFO : PROGRESS: pass 35, at document #100/100\n",
      "2020-02-19 20:05:30,979 : INFO : topic #0 (0.200): 0.000*\"bowl\" + 0.000*\"used used\" + 0.000*\"stress\" + 0.000*\"comparison rest\" + 0.000*\"cbd oil\" + 0.000*\"communicate\" + 0.000*\"providing\" + 0.000*\"wrong true\" + 0.000*\"chart\" + 0.000*\"truly\"\n",
      "2020-02-19 20:05:30,981 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:30,984 : INFO : topic #2 (0.200): 0.000*\"ve launched\" + 0.000*\"everybody thank\" + 0.000*\"november\" + 0.000*\"flow\" + 0.000*\"specie\" + 0.000*\"did played\" + 0.000*\"fact kind\" + 0.000*\"yes longer\" + 0.000*\"prison\" + 0.000*\"talking man\"\n",
      "2020-02-19 20:05:30,987 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"investable\" + 0.000*\"nutrient\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"come home\" + 0.000*\"don try\" + 0.000*\"bank transfer\" + 0.000*\"let don\"\n",
      "2020-02-19 20:05:30,989 : INFO : topic #4 (0.200): 0.000*\"thank thank\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"course didn\" + 0.000*\"versa\" + 0.000*\"pasta\" + 0.000*\"nate\" + 0.000*\"explained\" + 0.000*\"thinking eventually\" + 0.000*\"try yes\"\n",
      "2020-02-19 20:05:30,992 : INFO : topic diff=0.000004, rho=0.164399\n",
      "2020-02-19 20:05:34,041 : INFO : -14.597 per-word bound, 24780.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:34,042 : INFO : PROGRESS: pass 36, at document #100/100\n",
      "2020-02-19 20:05:34,494 : INFO : topic #0 (0.200): 0.000*\"comparison rest\" + 0.000*\"bowl\" + 0.000*\"used used\" + 0.000*\"wrong true\" + 0.000*\"cbd oil\" + 0.000*\"york state\" + 0.000*\"oh christ\" + 0.000*\"doing life\" + 0.000*\"liberal progressive\" + 0.000*\"sleep slept\"\n",
      "2020-02-19 20:05:34,496 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:34,498 : INFO : topic #2 (0.200): 0.000*\"ve launched\" + 0.000*\"did played\" + 0.000*\"fact kind\" + 0.000*\"yes longer\" + 0.000*\"november\" + 0.000*\"told wow\" + 0.000*\"jaw hit\" + 0.000*\"com gan\" + 0.000*\"started feeling\" + 0.000*\"okay end\"\n",
      "2020-02-19 20:05:34,501 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"nutrient\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"come home\" + 0.000*\"let don\" + 0.000*\"don try\" + 0.000*\"bank transfer\"\n",
      "2020-02-19 20:05:34,505 : INFO : topic #4 (0.200): 0.000*\"course didn\" + 0.000*\"versa\" + 0.000*\"pasta\" + 0.000*\"nate\" + 0.000*\"thank thank\" + 0.000*\"thinking eventually\" + 0.000*\"yes don\" + 0.000*\"counter\" + 0.000*\"try yes\" + 0.000*\"experience make\"\n",
      "2020-02-19 20:05:34,507 : INFO : topic diff=0.000003, rho=0.162221\n",
      "2020-02-19 20:05:37,554 : INFO : -14.597 per-word bound, 24780.1 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:37,555 : INFO : PROGRESS: pass 37, at document #100/100\n",
      "2020-02-19 20:05:38,040 : INFO : topic #0 (0.200): 0.000*\"comparison rest\" + 0.000*\"york state\" + 0.000*\"wrong true\" + 0.000*\"liberal progressive\" + 0.000*\"oh christ\" + 0.000*\"cbd oil\" + 0.000*\"sleep slept\" + 0.000*\"doing life\" + 0.000*\"went dark\" + 0.000*\"woman kid\"\n",
      "2020-02-19 20:05:38,042 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:38,045 : INFO : topic #2 (0.200): 0.000*\"ve launched\" + 0.000*\"did played\" + 0.000*\"com gan\" + 0.000*\"fact kind\" + 0.000*\"jaw hit\" + 0.000*\"yes longer\" + 0.000*\"told wow\" + 0.000*\"okay end\" + 0.000*\"started feeling\" + 0.000*\"hit floor\"\n",
      "2020-02-19 20:05:38,049 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"nutrient\" + 0.000*\"investable\" + 0.000*\"experience train\" + 0.000*\"doe make\" + 0.000*\"come home\" + 0.000*\"let don\" + 0.000*\"don try\" + 0.000*\"bank transfer\"\n",
      "2020-02-19 20:05:38,052 : INFO : topic #4 (0.200): 0.000*\"course didn\" + 0.000*\"thinking eventually\" + 0.000*\"versa\" + 0.000*\"nate\" + 0.000*\"experience make\" + 0.000*\"try yes\" + 0.000*\"course yes\" + 0.000*\"pasta\" + 0.000*\"rotate\" + 0.000*\"civilization culture\"\n",
      "2020-02-19 20:05:38,054 : INFO : topic diff=0.000003, rho=0.160128\n",
      "2020-02-19 20:05:41,084 : INFO : -14.597 per-word bound, 24780.1 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:41,084 : INFO : PROGRESS: pass 38, at document #100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 20:05:41,537 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"york state\" + 0.000*\"comparison rest\" + 0.000*\"wrong true\" + 0.000*\"oh christ\" + 0.000*\"went dark\" + 0.000*\"movement thank\" + 0.000*\"thinking little\" + 0.000*\"hmm called\" + 0.000*\"sleep slept\"\n",
      "2020-02-19 20:05:41,539 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:41,543 : INFO : topic #2 (0.200): 0.000*\"ve launched\" + 0.000*\"com gan\" + 0.000*\"did played\" + 0.000*\"fact kind\" + 0.000*\"yes longer\" + 0.000*\"jaw hit\" + 0.000*\"told wow\" + 0.000*\"started feeling\" + 0.000*\"okay end\" + 0.000*\"hit floor\"\n",
      "2020-02-19 20:05:41,546 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"nutrient\" + 0.000*\"experience train\" + 0.000*\"investable\" + 0.000*\"come home\" + 0.000*\"doe make\" + 0.000*\"let don\" + 0.000*\"revolutionary\" + 0.000*\"bank transfer\"\n",
      "2020-02-19 20:05:41,549 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"course didn\" + 0.000*\"try yes\" + 0.000*\"experience make\" + 0.000*\"versa\" + 0.000*\"course yes\" + 0.000*\"civilization culture\" + 0.000*\"rotate\" + 0.000*\"nate\" + 0.000*\"yes maybe\"\n",
      "2020-02-19 20:05:41,551 : INFO : topic diff=0.000002, rho=0.158114\n",
      "2020-02-19 20:05:44,593 : INFO : -14.597 per-word bound, 24780.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:44,594 : INFO : PROGRESS: pass 39, at document #100/100\n",
      "2020-02-19 20:05:45,052 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"york state\" + 0.000*\"comparison rest\" + 0.000*\"wrong true\" + 0.000*\"went dark\" + 0.000*\"oh christ\" + 0.000*\"thinking little\" + 0.000*\"relaxation\" + 0.000*\"person belief\" + 0.000*\"preface\"\n",
      "2020-02-19 20:05:45,054 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:45,058 : INFO : topic #2 (0.200): 0.000*\"ve launched\" + 0.000*\"com gan\" + 0.000*\"did played\" + 0.000*\"fact kind\" + 0.000*\"yes longer\" + 0.000*\"yes amazing\" + 0.000*\"liberal progressive\" + 0.000*\"started feeling\" + 0.000*\"qualified candidate\" + 0.000*\"okay end\"\n",
      "2020-02-19 20:05:45,060 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"nutrient\" + 0.000*\"experience train\" + 0.000*\"revolutionary\" + 0.000*\"investable\" + 0.000*\"come home\" + 0.000*\"doe make\" + 0.000*\"let don\" + 0.000*\"don try\"\n",
      "2020-02-19 20:05:45,063 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"course didn\" + 0.000*\"experience make\" + 0.000*\"try yes\" + 0.000*\"course yes\" + 0.000*\"rotate\" + 0.000*\"versa\" + 0.000*\"civilization culture\" + 0.000*\"yes maybe\" + 0.000*\"holland\"\n",
      "2020-02-19 20:05:45,065 : INFO : topic diff=0.000002, rho=0.156174\n",
      "2020-02-19 20:05:48,128 : INFO : -14.597 per-word bound, 24780.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:48,129 : INFO : PROGRESS: pass 40, at document #100/100\n",
      "2020-02-19 20:05:48,579 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"york state\" + 0.000*\"wrong true\" + 0.000*\"comparison rest\" + 0.000*\"preface\" + 0.000*\"thinking little\" + 0.000*\"hmm called\" + 0.000*\"person belief\" + 0.000*\"went dark\" + 0.000*\"ridiculous group\"\n",
      "2020-02-19 20:05:48,581 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:48,584 : INFO : topic #2 (0.200): 0.000*\"ve launched\" + 0.000*\"com gan\" + 0.000*\"liberal progressive\" + 0.000*\"yes longer\" + 0.000*\"yes amazing\" + 0.000*\"fact kind\" + 0.000*\"qualified candidate\" + 0.000*\"did played\" + 0.000*\"mean care\" + 0.000*\"jaw hit\"\n",
      "2020-02-19 20:05:48,588 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"revolutionary\" + 0.000*\"nutrient\" + 0.000*\"investable\" + 0.000*\"effective way\" + 0.000*\"experience train\" + 0.000*\"come home\" + 0.000*\"let don\" + 0.000*\"doe make\"\n",
      "2020-02-19 20:05:48,590 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"experience make\" + 0.000*\"course didn\" + 0.000*\"try yes\" + 0.000*\"civilization culture\" + 0.000*\"rotate\" + 0.000*\"course yes\" + 0.000*\"versa\" + 0.000*\"today weird\" + 0.000*\"yes maybe\"\n",
      "2020-02-19 20:05:48,592 : INFO : topic diff=0.000002, rho=0.154303\n",
      "2020-02-19 20:05:51,626 : INFO : -14.597 per-word bound, 24780.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:51,627 : INFO : PROGRESS: pass 41, at document #100/100\n",
      "2020-02-19 20:05:52,076 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"york state\" + 0.000*\"preface\" + 0.000*\"sort horrible\" + 0.000*\"person belief\" + 0.000*\"angry work\" + 0.000*\"thinking little\" + 0.000*\"wrong true\" + 0.000*\"went dark\" + 0.000*\"sip recruiter\"\n",
      "2020-02-19 20:05:52,078 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:52,083 : INFO : topic #2 (0.200): 0.000*\"ve launched\" + 0.000*\"com gan\" + 0.000*\"liberal progressive\" + 0.000*\"preface\" + 0.000*\"joni\" + 0.000*\"qualified candidate\" + 0.000*\"yes amazing\" + 0.000*\"fact kind\" + 0.000*\"gan zip\" + 0.000*\"job zip\"\n",
      "2020-02-19 20:05:52,088 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"app second\" + 0.000*\"revolutionary\" + 0.000*\"effective way\" + 0.000*\"nutrient\" + 0.000*\"woman saying\" + 0.000*\"come home\" + 0.000*\"investable\" + 0.000*\"let don\" + 0.000*\"experience train\"\n",
      "2020-02-19 20:05:52,090 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"experience make\" + 0.000*\"course didn\" + 0.000*\"civilization culture\" + 0.000*\"try yes\" + 0.000*\"rotate\" + 0.000*\"course yes\" + 0.000*\"today weird\" + 0.000*\"aunt great\" + 0.000*\"yes maybe\"\n",
      "2020-02-19 20:05:52,092 : INFO : topic diff=0.000002, rho=0.152499\n",
      "2020-02-19 20:05:55,131 : INFO : -14.597 per-word bound, 24780.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:55,132 : INFO : PROGRESS: pass 42, at document #100/100\n",
      "2020-02-19 20:05:55,586 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"york state\" + 0.000*\"person belief\" + 0.000*\"preface\" + 0.000*\"angry work\" + 0.000*\"sort horrible\" + 0.000*\"ridiculous group\" + 0.000*\"sip recruiter\" + 0.000*\"act wasn\" + 0.000*\"belief ridiculous\"\n",
      "2020-02-19 20:05:55,589 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:55,593 : INFO : topic #2 (0.200): 0.000*\"com gan\" + 0.000*\"ve launched\" + 0.000*\"liberal progressive\" + 0.000*\"preface\" + 0.000*\"qualified candidate\" + 0.000*\"joni\" + 0.000*\"yes amazing\" + 0.000*\"job zip\" + 0.000*\"gan zip\" + 0.000*\"fact kind\"\n",
      "2020-02-19 20:05:55,596 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"revolutionary\" + 0.000*\"woman saying\" + 0.000*\"effective way\" + 0.000*\"app second\" + 0.000*\"knew mean\" + 0.000*\"stretched\" + 0.000*\"nutrient\" + 0.000*\"feel depressed\" + 0.000*\"come home\"\n",
      "2020-02-19 20:05:55,599 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"experience make\" + 0.000*\"civilization culture\" + 0.000*\"course yes\" + 0.000*\"course didn\" + 0.000*\"try yes\" + 0.000*\"rotate\" + 0.000*\"today weird\" + 0.000*\"teaching teacher\" + 0.000*\"aunt great\"\n",
      "2020-02-19 20:05:55,602 : INFO : topic diff=0.000001, rho=0.150756\n",
      "2020-02-19 20:05:58,681 : INFO : -14.597 per-word bound, 24780.3 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:05:58,682 : INFO : PROGRESS: pass 43, at document #100/100\n",
      "2020-02-19 20:05:59,152 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"ridiculous group\" + 0.000*\"person belief\" + 0.000*\"angry work\" + 0.000*\"york state\" + 0.000*\"preface\" + 0.000*\"sort horrible\" + 0.000*\"sip recruiter\" + 0.000*\"recruiter zipper\" + 0.000*\"day zip\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 20:05:59,156 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:05:59,159 : INFO : topic #2 (0.200): 0.000*\"com gan\" + 0.000*\"liberal progressive\" + 0.000*\"ve launched\" + 0.000*\"preface\" + 0.000*\"joni\" + 0.000*\"yes amazing\" + 0.000*\"belief sort\" + 0.000*\"ridiculous group\" + 0.000*\"qualified candidate\" + 0.000*\"history sort\"\n",
      "2020-02-19 20:05:59,162 : INFO : topic #3 (0.200): 0.000*\"critical\" + 0.000*\"woman saying\" + 0.000*\"revolutionary\" + 0.000*\"effective way\" + 0.000*\"knew mean\" + 0.000*\"stretched\" + 0.000*\"feel depressed\" + 0.000*\"understand body\" + 0.000*\"book come\" + 0.000*\"generalizing\"\n",
      "2020-02-19 20:05:59,165 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"experience make\" + 0.000*\"civilization culture\" + 0.000*\"try yes\" + 0.000*\"rotate\" + 0.000*\"course yes\" + 0.000*\"today weird\" + 0.000*\"course didn\" + 0.000*\"aunt great\" + 0.000*\"teaching teacher\"\n",
      "2020-02-19 20:05:59,167 : INFO : topic diff=0.000001, rho=0.149071\n",
      "2020-02-19 20:06:02,215 : INFO : -14.597 per-word bound, 24780.3 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:06:02,216 : INFO : PROGRESS: pass 44, at document #100/100\n",
      "2020-02-19 20:06:02,687 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"preface\" + 0.000*\"angry work\" + 0.000*\"ridiculous group\" + 0.000*\"person belief\" + 0.000*\"sort horrible\" + 0.000*\"york state\" + 0.000*\"incredible number\" + 0.000*\"act wasn\" + 0.000*\"country especially\"\n",
      "2020-02-19 20:06:02,690 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:06:02,693 : INFO : topic #2 (0.200): 0.000*\"liberal progressive\" + 0.000*\"com gan\" + 0.000*\"ve launched\" + 0.000*\"preface\" + 0.000*\"history sort\" + 0.000*\"joni\" + 0.000*\"ridiculous group\" + 0.000*\"angry work\" + 0.000*\"yes amazing\" + 0.000*\"doing play\"\n",
      "2020-02-19 20:06:02,696 : INFO : topic #3 (0.200): 0.000*\"woman saying\" + 0.000*\"feel depressed\" + 0.000*\"knew mean\" + 0.000*\"understand body\" + 0.000*\"stretched\" + 0.000*\"revolutionary\" + 0.000*\"effective way\" + 0.000*\"diminished\" + 0.000*\"critical\" + 0.000*\"city live\"\n",
      "2020-02-19 20:06:02,700 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"experience make\" + 0.000*\"civilization culture\" + 0.000*\"rotate\" + 0.000*\"try yes\" + 0.000*\"course yes\" + 0.000*\"today weird\" + 0.000*\"sort horrible\" + 0.000*\"teaching teacher\" + 0.000*\"course didn\"\n",
      "2020-02-19 20:06:02,701 : INFO : topic diff=0.000001, rho=0.147442\n",
      "2020-02-19 20:06:05,731 : INFO : -14.597 per-word bound, 24780.1 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:06:05,731 : INFO : PROGRESS: pass 45, at document #100/100\n",
      "2020-02-19 20:06:06,185 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"preface\" + 0.000*\"angry work\" + 0.000*\"ridiculous group\" + 0.000*\"person belief\" + 0.000*\"sort horrible\" + 0.000*\"country especially\" + 0.000*\"york state\" + 0.000*\"sip recruiter\" + 0.000*\"act wasn\"\n",
      "2020-02-19 20:06:06,188 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:06:06,192 : INFO : topic #2 (0.200): 0.000*\"liberal progressive\" + 0.000*\"com gan\" + 0.000*\"preface\" + 0.000*\"ridiculous group\" + 0.000*\"ve launched\" + 0.000*\"angry work\" + 0.000*\"history sort\" + 0.000*\"joni\" + 0.000*\"yes amazing\" + 0.000*\"qualified candidate\"\n",
      "2020-02-19 20:06:06,194 : INFO : topic #3 (0.200): 0.000*\"feel depressed\" + 0.000*\"understand body\" + 0.000*\"woman saying\" + 0.000*\"knew mean\" + 0.000*\"stretched\" + 0.000*\"diminished\" + 0.000*\"preface\" + 0.000*\"city live\" + 0.000*\"man called\" + 0.000*\"editorial\"\n",
      "2020-02-19 20:06:06,197 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"experience make\" + 0.000*\"civilization culture\" + 0.000*\"rotate\" + 0.000*\"sort horrible\" + 0.000*\"today weird\" + 0.000*\"aunt great\" + 0.000*\"course yes\" + 0.000*\"try yes\" + 0.000*\"teaching teacher\"\n",
      "2020-02-19 20:06:06,199 : INFO : topic diff=0.000001, rho=0.145865\n",
      "2020-02-19 20:06:09,253 : INFO : -14.597 per-word bound, 24780.5 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:06:09,253 : INFO : PROGRESS: pass 46, at document #100/100\n",
      "2020-02-19 20:06:09,707 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"preface\" + 0.000*\"angry work\" + 0.000*\"ridiculous group\" + 0.000*\"act wasn\" + 0.000*\"sip recruiter\" + 0.000*\"person belief\" + 0.000*\"sort horrible\" + 0.000*\"recruiter zipper\" + 0.000*\"country especially\"\n",
      "2020-02-19 20:06:09,709 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:06:09,712 : INFO : topic #2 (0.200): 0.000*\"liberal progressive\" + 0.000*\"preface\" + 0.000*\"com gan\" + 0.000*\"ridiculous group\" + 0.000*\"angry work\" + 0.000*\"joni\" + 0.000*\"history sort\" + 0.000*\"doing play\" + 0.000*\"sort feeling\" + 0.000*\"thinking eventually\"\n",
      "2020-02-19 20:06:09,714 : INFO : topic #3 (0.200): 0.000*\"understand body\" + 0.000*\"feel depressed\" + 0.000*\"preface\" + 0.000*\"knew mean\" + 0.000*\"woman saying\" + 0.000*\"diminished\" + 0.000*\"editorial\" + 0.000*\"stretched\" + 0.000*\"city live\" + 0.000*\"man called\"\n",
      "2020-02-19 20:06:09,717 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"experience make\" + 0.000*\"civilization culture\" + 0.000*\"aunt great\" + 0.000*\"rotate\" + 0.000*\"today weird\" + 0.000*\"sort horrible\" + 0.000*\"ridiculous group\" + 0.000*\"liberal progressive\" + 0.000*\"course yes\"\n",
      "2020-02-19 20:06:09,719 : INFO : topic diff=0.000001, rho=0.144338\n",
      "2020-02-19 20:06:12,748 : INFO : -14.597 per-word bound, 24780.4 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:06:12,749 : INFO : PROGRESS: pass 47, at document #100/100\n",
      "2020-02-19 20:06:13,199 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"ridiculous group\" + 0.000*\"angry work\" + 0.000*\"preface\" + 0.000*\"sort horrible\" + 0.000*\"sip recruiter\" + 0.000*\"person belief\" + 0.000*\"act wasn\" + 0.000*\"vote gay\" + 0.000*\"belief ridiculous\"\n",
      "2020-02-19 20:06:13,201 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:06:13,204 : INFO : topic #2 (0.200): 0.000*\"preface\" + 0.000*\"liberal progressive\" + 0.000*\"com gan\" + 0.000*\"ridiculous group\" + 0.000*\"angry work\" + 0.000*\"history sort\" + 0.000*\"thinking eventually\" + 0.000*\"joni\" + 0.000*\"sort horrible\" + 0.000*\"wonder ve\"\n",
      "2020-02-19 20:06:13,209 : INFO : topic #3 (0.200): 0.000*\"understand body\" + 0.000*\"preface\" + 0.000*\"feel depressed\" + 0.000*\"diminished\" + 0.000*\"previous book\" + 0.000*\"knew mean\" + 0.000*\"sort feeling\" + 0.000*\"editorial\" + 0.000*\"man called\" + 0.000*\"woman saying\"\n",
      "2020-02-19 20:06:13,211 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"civilization culture\" + 0.000*\"experience make\" + 0.000*\"today weird\" + 0.000*\"sort horrible\" + 0.000*\"liberal progressive\" + 0.000*\"angry work\" + 0.000*\"teaching teacher\" + 0.000*\"rotate\" + 0.000*\"aunt great\"\n",
      "2020-02-19 20:06:13,213 : INFO : topic diff=0.000001, rho=0.142857\n",
      "2020-02-19 20:06:16,255 : INFO : -14.597 per-word bound, 24780.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:06:16,256 : INFO : PROGRESS: pass 48, at document #100/100\n",
      "2020-02-19 20:06:16,711 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"preface\" + 0.000*\"ridiculous group\" + 0.000*\"angry work\" + 0.000*\"sip recruiter\" + 0.000*\"sort horrible\" + 0.000*\"person belief\" + 0.000*\"country especially\" + 0.000*\"act wasn\" + 0.000*\"recruiter zipper\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 20:06:16,713 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:06:16,716 : INFO : topic #2 (0.200): 0.000*\"preface\" + 0.000*\"liberal progressive\" + 0.000*\"ridiculous group\" + 0.000*\"angry work\" + 0.000*\"com gan\" + 0.000*\"sort horrible\" + 0.000*\"joni\" + 0.000*\"history sort\" + 0.000*\"thinking eventually\" + 0.000*\"sip recruiter\"\n",
      "2020-02-19 20:06:16,719 : INFO : topic #3 (0.200): 0.000*\"understand body\" + 0.000*\"preface\" + 0.000*\"feel depressed\" + 0.000*\"angry work\" + 0.000*\"editorial\" + 0.000*\"sort feeling\" + 0.000*\"ridiculous group\" + 0.000*\"liberal progressive\" + 0.000*\"wallet cash\" + 0.000*\"previous book\"\n",
      "2020-02-19 20:06:16,724 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"experience make\" + 0.000*\"civilization culture\" + 0.000*\"ridiculous group\" + 0.000*\"liberal progressive\" + 0.000*\"today weird\" + 0.000*\"angry work\" + 0.000*\"sort horrible\" + 0.000*\"teaching teacher\" + 0.000*\"preface\"\n",
      "2020-02-19 20:06:16,726 : INFO : topic diff=0.000001, rho=0.141421\n",
      "2020-02-19 20:06:19,770 : INFO : -14.597 per-word bound, 24780.2 perplexity estimate based on a held-out corpus of 100 documents with 6647 words\n",
      "2020-02-19 20:06:19,771 : INFO : PROGRESS: pass 49, at document #100/100\n",
      "2020-02-19 20:06:20,222 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"angry work\" + 0.000*\"ridiculous group\" + 0.000*\"preface\" + 0.000*\"sip recruiter\" + 0.000*\"person belief\" + 0.000*\"act wasn\" + 0.000*\"sort horrible\" + 0.000*\"country especially\" + 0.000*\"incredible number\"\n",
      "2020-02-19 20:06:20,224 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:06:20,227 : INFO : topic #2 (0.200): 0.000*\"ridiculous group\" + 0.000*\"angry work\" + 0.000*\"preface\" + 0.000*\"liberal progressive\" + 0.000*\"com gan\" + 0.000*\"thinking eventually\" + 0.000*\"joni\" + 0.000*\"history sort\" + 0.000*\"sip recruiter\" + 0.000*\"person belief\"\n",
      "2020-02-19 20:06:20,232 : INFO : topic #3 (0.200): 0.000*\"preface\" + 0.000*\"understand body\" + 0.000*\"ridiculous group\" + 0.000*\"angry work\" + 0.000*\"liberal progressive\" + 0.000*\"editorial\" + 0.000*\"person belief\" + 0.000*\"sort feeling\" + 0.000*\"feel depressed\" + 0.000*\"did address\"\n",
      "2020-02-19 20:06:20,235 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"ridiculous group\" + 0.000*\"experience make\" + 0.000*\"civilization culture\" + 0.000*\"angry work\" + 0.000*\"liberal progressive\" + 0.000*\"sort horrible\" + 0.000*\"aunt great\" + 0.000*\"today weird\" + 0.000*\"rotate\"\n",
      "2020-02-19 20:06:20,237 : INFO : topic diff=0.000000, rho=0.140028\n"
     ]
    }
   ],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "if run_lda:    \n",
    "    lda = models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 20:06:20,331 : INFO : topic #0 (0.200): 0.000*\"liberal progressive\" + 0.000*\"angry work\" + 0.000*\"ridiculous group\" + 0.000*\"preface\" + 0.000*\"sip recruiter\" + 0.000*\"person belief\" + 0.000*\"act wasn\" + 0.000*\"sort horrible\" + 0.000*\"country especially\" + 0.000*\"incredible number\"\n",
      "2020-02-19 20:06:20,334 : INFO : topic #1 (0.200): 0.000*\"train day\" + 0.000*\"application\" + 0.000*\"approach\" + 0.000*\"truly\" + 0.000*\"invest\" + 0.000*\"experience train\" + 0.000*\"building pygmy\" + 0.000*\"charity building\" + 0.000*\"summer\" + 0.000*\"way send\"\n",
      "2020-02-19 20:06:20,337 : INFO : topic #2 (0.200): 0.000*\"ridiculous group\" + 0.000*\"angry work\" + 0.000*\"preface\" + 0.000*\"liberal progressive\" + 0.000*\"com gan\" + 0.000*\"thinking eventually\" + 0.000*\"joni\" + 0.000*\"history sort\" + 0.000*\"sip recruiter\" + 0.000*\"person belief\"\n",
      "2020-02-19 20:06:20,340 : INFO : topic #3 (0.200): 0.000*\"preface\" + 0.000*\"understand body\" + 0.000*\"ridiculous group\" + 0.000*\"angry work\" + 0.000*\"liberal progressive\" + 0.000*\"editorial\" + 0.000*\"person belief\" + 0.000*\"sort feeling\" + 0.000*\"feel depressed\" + 0.000*\"did address\"\n",
      "2020-02-19 20:06:20,343 : INFO : topic #4 (0.200): 0.000*\"thinking eventually\" + 0.000*\"ridiculous group\" + 0.000*\"experience make\" + 0.000*\"civilization culture\" + 0.000*\"angry work\" + 0.000*\"liberal progressive\" + 0.000*\"sort horrible\" + 0.000*\"aunt great\" + 0.000*\"today weird\" + 0.000*\"rotate\"\n"
     ]
    }
   ],
   "source": [
    "if run_lda:\n",
    "    lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 108673)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word = ps_ep_list_tokenizer.transform(ps_ep_list)\n",
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00150386, 0.01553777, 0.0119959 , 0.01179166, 0.01135285])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "#  Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "lsa = TruncatedSVD(num_topics)\n",
    "doc_topic = lsa.fit_transform(doc_word)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa meeting</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aav</th>\n",
       "      <th>aavs</th>\n",
       "      <th>ab</th>\n",
       "      <th>abalone</th>\n",
       "      <th>abandon</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom zoom</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zu</th>\n",
       "      <th>zubi</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuckerberg facebook</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_5</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                aa  aa meeting    aaa    aah  aaron    aav   aavs     ab  \\\n",
       "component_1  0.005       0.001  0.001  0.002  0.005  0.002  0.002  0.007   \n",
       "component_2  0.002      -0.002 -0.001 -0.000 -0.002  0.003  0.003 -0.002   \n",
       "component_3  0.001       0.001 -0.002 -0.001 -0.004  0.010  0.012  0.004   \n",
       "component_4  0.005       0.001 -0.003 -0.001  0.005 -0.008 -0.008  0.004   \n",
       "component_5 -0.001      -0.000  0.005  0.001  0.005 -0.001 -0.001 -0.006   \n",
       "\n",
       "             abalone  abandon  ...  zoom zoom  zoomed  zooming     zu   zubi  \\\n",
       "component_1    0.001    0.006  ...      0.003   0.002    0.001  0.001  0.002   \n",
       "component_2    0.001    0.006  ...      0.001  -0.002    0.002  0.001 -0.001   \n",
       "component_3   -0.001   -0.003  ...     -0.004   0.002    0.002 -0.005 -0.002   \n",
       "component_4   -0.003    0.000  ...     -0.000  -0.004    0.005 -0.002 -0.000   \n",
       "component_5    0.000   -0.002  ...     -0.002   0.002   -0.004 -0.003  0.001   \n",
       "\n",
       "             zucchini  zuckerberg  zuckerberg facebook  zumba     zz  \n",
       "component_1     0.001       0.005                0.001  0.001  0.002  \n",
       "component_2    -0.002       0.009                0.005 -0.003 -0.004  \n",
       "component_3     0.001      -0.006               -0.003  0.002  0.002  \n",
       "component_4     0.002      -0.003                0.001 -0.002 -0.000  \n",
       "component_5     0.001      -0.004               -0.001  0.000  0.003  \n",
       "\n",
       "[5 rows x 108673 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = col_names,\n",
    "             columns = ps_ep_list_tokenizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "summer, way send, support good, everybody thank, invest\n",
      "\n",
      "Topic  1\n",
      "economic, revolution, radically, entity, transparent\n",
      "\n",
      "Topic  2\n",
      "brought expressvpn, expressvpn, expressvpn com, learn brought, computer phone\n",
      "\n",
      "Topic  3\n",
      "animal product, highest quality, harmful chemical, psychological, formulation\n",
      "\n",
      "Topic  4\n",
      "sleep make, use code, loop strap, wearing whoop, card fee\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, ps_ep_list_tokenizer.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "      <th>component_3</th>\n",
       "      <th>component_4</th>\n",
       "      <th>component_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#1383 - Malcolm Gladwell</th>\n",
       "      <td>0.26906</td>\n",
       "      <td>0.03806</td>\n",
       "      <td>-0.02832</td>\n",
       "      <td>0.05508</td>\n",
       "      <td>-0.08561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1382 - RZA &amp; Donnell Rawlings</th>\n",
       "      <td>0.25006</td>\n",
       "      <td>-0.06743</td>\n",
       "      <td>-0.01837</td>\n",
       "      <td>-0.10845</td>\n",
       "      <td>-0.13158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1378 - Greg Fitzsimmons</th>\n",
       "      <td>0.27514</td>\n",
       "      <td>-0.05180</td>\n",
       "      <td>-0.03012</td>\n",
       "      <td>-0.09914</td>\n",
       "      <td>0.04494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1375 - Edward Norton</th>\n",
       "      <td>0.19524</td>\n",
       "      <td>-0.02456</td>\n",
       "      <td>0.05802</td>\n",
       "      <td>0.02960</td>\n",
       "      <td>-0.02234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1373 - Kyle Kulinski</th>\n",
       "      <td>0.27395</td>\n",
       "      <td>0.23410</td>\n",
       "      <td>-0.10398</td>\n",
       "      <td>-0.02188</td>\n",
       "      <td>0.11881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1280 - Michael Yo</th>\n",
       "      <td>0.29219</td>\n",
       "      <td>-0.08155</td>\n",
       "      <td>-0.03396</td>\n",
       "      <td>0.01267</td>\n",
       "      <td>-0.09156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1279 - Jessimae Peluso</th>\n",
       "      <td>0.27107</td>\n",
       "      <td>-0.05227</td>\n",
       "      <td>-0.03853</td>\n",
       "      <td>-0.04787</td>\n",
       "      <td>-0.10910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1278 - Kevin Hart</th>\n",
       "      <td>0.21349</td>\n",
       "      <td>-0.03098</td>\n",
       "      <td>-0.03489</td>\n",
       "      <td>0.07325</td>\n",
       "      <td>-0.03564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1277 - Gabrielle Reece</th>\n",
       "      <td>0.24610</td>\n",
       "      <td>-0.02092</td>\n",
       "      <td>-0.12910</td>\n",
       "      <td>0.10405</td>\n",
       "      <td>-0.15772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1276 - Ben Shapiro</th>\n",
       "      <td>0.24307</td>\n",
       "      <td>0.16080</td>\n",
       "      <td>-0.12708</td>\n",
       "      <td>0.06640</td>\n",
       "      <td>-0.01932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 component_1  component_2  component_3  \\\n",
       " #1383 - Malcolm Gladwell            0.26906      0.03806     -0.02832   \n",
       " #1382 - RZA & Donnell Rawlings      0.25006     -0.06743     -0.01837   \n",
       " #1378 - Greg Fitzsimmons            0.27514     -0.05180     -0.03012   \n",
       " #1375 - Edward Norton               0.19524     -0.02456      0.05802   \n",
       " #1373 - Kyle Kulinski               0.27395      0.23410     -0.10398   \n",
       "...                                      ...          ...          ...   \n",
       " #1280 - Michael Yo                  0.29219     -0.08155     -0.03396   \n",
       " #1279 - Jessimae Peluso             0.27107     -0.05227     -0.03853   \n",
       " #1278 - Kevin Hart                  0.21349     -0.03098     -0.03489   \n",
       " #1277 - Gabrielle Reece             0.24610     -0.02092     -0.12910   \n",
       " #1276 - Ben Shapiro                 0.24307      0.16080     -0.12708   \n",
       "\n",
       "                                 component_4  component_5  \n",
       " #1383 - Malcolm Gladwell            0.05508     -0.08561  \n",
       " #1382 - RZA & Donnell Rawlings     -0.10845     -0.13158  \n",
       " #1378 - Greg Fitzsimmons           -0.09914      0.04494  \n",
       " #1375 - Edward Norton               0.02960     -0.02234  \n",
       " #1373 - Kyle Kulinski              -0.02188      0.11881  \n",
       "...                                      ...          ...  \n",
       " #1280 - Michael Yo                  0.01267     -0.09156  \n",
       " #1279 - Jessimae Peluso            -0.04787     -0.10910  \n",
       " #1278 - Kevin Hart                  0.07325     -0.03564  \n",
       " #1277 - Gabrielle Reece             0.10405     -0.15772  \n",
       " #1276 - Ben Shapiro                 0.06640     -0.01932  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vt = pd.DataFrame(doc_topic.round(5),\n",
    "             index = ps_ep_names,\n",
    "             columns = col_names)\n",
    "Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word = ps_ep_list_tokenizer.transform(ps_ep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(num_topics)\n",
    "doc_topic = nmf_model.fit_transform(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa meeting</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aav</th>\n",
       "      <th>aavs</th>\n",
       "      <th>ab</th>\n",
       "      <th>abalone</th>\n",
       "      <th>abandon</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom zoom</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zu</th>\n",
       "      <th>zubi</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuckerberg facebook</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_5</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                aa  aa meeting    aaa    aah  aaron    aav   aavs     ab  \\\n",
       "component_1  0.008       0.004  0.004  0.003  0.008  0.000  0.001  0.012   \n",
       "component_2  0.006       0.000  0.000  0.002  0.002  0.000  0.000  0.002   \n",
       "component_3  0.000       0.000  0.000  0.000  0.000  0.000  0.002  0.006   \n",
       "component_4  0.000       0.000  0.000  0.001  0.010  0.000  0.000  0.000   \n",
       "component_5  0.001       0.000  0.000  0.001  0.000  0.018  0.017  0.006   \n",
       "\n",
       "             abalone  abandon  ...  zoom zoom  zoomed  zooming     zu   zubi  \\\n",
       "component_1    0.001    0.006  ...      0.005   0.003    0.001  0.001  0.004   \n",
       "component_2    0.002    0.009  ...      0.004   0.000    0.003  0.003  0.001   \n",
       "component_3    0.000    0.002  ...      0.000   0.000    0.000  0.000  0.000   \n",
       "component_4    0.001    0.000  ...      0.000   0.000    0.000  0.000  0.000   \n",
       "component_5    0.001    0.003  ...      0.000   0.008    0.000  0.000  0.000   \n",
       "\n",
       "             zucchini  zuckerberg  zuckerberg facebook  zumba     zz  \n",
       "component_1     0.004       0.003                0.000  0.003  0.006  \n",
       "component_2     0.000       0.014                0.006  0.000  0.000  \n",
       "component_3     0.000       0.001                0.000  0.000  0.000  \n",
       "component_4     0.000       0.000                0.000  0.004  0.000  \n",
       "component_5     0.000       0.000                0.000  0.000  0.000  \n",
       "\n",
       "[5 rows x 108673 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = col_names,\n",
    "             columns = ps_ep_list_tokenizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "ate, workout, vega, jitsu, joey, oh sure, friday, yoga, girlfriend, jacked\n",
      "\n",
      "Topic  1\n",
      "transparent, economic, discussion, decision making, bias, citizen, democracy, disagree, donald trump, democrat\n",
      "\n",
      "Topic  2\n",
      "offer boost, packed premium, reward come, come packed, instant reward, premium feature, card offer, feature credit, place check, required instant\n",
      "\n",
      "Topic  3\n",
      "recruiter need, cooter scan, technology zipper, job web, leading job, site day, candidate miss, analyzes spotlight, thousand resume, candidate site\n",
      "\n",
      "Topic  4\n",
      "mattock company, kind went, kind said, mane chaga, aircraft, government doesn, mushroom elixir, refresh, mane mushroom, fo\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, ps_ep_list_tokenizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "      <th>component_3</th>\n",
       "      <th>component_4</th>\n",
       "      <th>component_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#1383 - Malcolm Gladwell</th>\n",
       "      <td>0.07879</td>\n",
       "      <td>0.17290</td>\n",
       "      <td>0.01023</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1382 - RZA &amp; Donnell Rawlings</th>\n",
       "      <td>0.14471</td>\n",
       "      <td>0.00905</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02022</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1378 - Greg Fitzsimmons</th>\n",
       "      <td>0.14364</td>\n",
       "      <td>0.05576</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00082</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1375 - Edward Norton</th>\n",
       "      <td>0.10763</td>\n",
       "      <td>0.00978</td>\n",
       "      <td>0.01659</td>\n",
       "      <td>0.00215</td>\n",
       "      <td>0.00569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1373 - Kyle Kulinski</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.39448</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1280 - Michael Yo</th>\n",
       "      <td>0.16886</td>\n",
       "      <td>0.02438</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1279 - Jessimae Peluso</th>\n",
       "      <td>0.14096</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00573</td>\n",
       "      <td>0.09631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1278 - Kevin Hart</th>\n",
       "      <td>0.11866</td>\n",
       "      <td>0.02490</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00321</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1277 - Gabrielle Reece</th>\n",
       "      <td>0.12894</td>\n",
       "      <td>0.03367</td>\n",
       "      <td>0.01463</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1276 - Ben Shapiro</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.32834</td>\n",
       "      <td>0.00201</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 component_1  component_2  component_3  \\\n",
       " #1383 - Malcolm Gladwell            0.07879      0.17290      0.01023   \n",
       " #1382 - RZA & Donnell Rawlings      0.14471      0.00905      0.00000   \n",
       " #1378 - Greg Fitzsimmons            0.14364      0.05576      0.00000   \n",
       " #1375 - Edward Norton               0.10763      0.00978      0.01659   \n",
       " #1373 - Kyle Kulinski               0.00000      0.39448      0.00000   \n",
       "...                                      ...          ...          ...   \n",
       " #1280 - Michael Yo                  0.16886      0.02438      0.00000   \n",
       " #1279 - Jessimae Peluso             0.14096      0.00000      0.00000   \n",
       " #1278 - Kevin Hart                  0.11866      0.02490      0.00000   \n",
       " #1277 - Gabrielle Reece             0.12894      0.03367      0.01463   \n",
       " #1276 - Ben Shapiro                 0.00000      0.32834      0.00201   \n",
       "\n",
       "                                 component_4  component_5  \n",
       " #1383 - Malcolm Gladwell            0.00000      0.00000  \n",
       " #1382 - RZA & Donnell Rawlings      0.02022      0.00000  \n",
       " #1378 - Greg Fitzsimmons            0.00082      0.00000  \n",
       " #1375 - Edward Norton               0.00215      0.00569  \n",
       " #1373 - Kyle Kulinski               0.00000      0.00000  \n",
       "...                                      ...          ...  \n",
       " #1280 - Michael Yo                  0.00000      0.00000  \n",
       " #1279 - Jessimae Peluso             0.00573      0.09631  \n",
       " #1278 - Kevin Hart                  0.00321      0.00000  \n",
       " #1277 - Gabrielle Reece             0.00000      0.00238  \n",
       " #1276 - Ben Shapiro                 0.00000      0.00000  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = pd.DataFrame(doc_topic.round(5),\n",
    "             index = ps_ep_names,\n",
    "             columns = col_names)\n",
    "H"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
