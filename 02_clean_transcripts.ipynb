{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python Version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# auto reload imports that change\n",
    "%load_ext autoreload\n",
    "# only set to auto reload for marked imports\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "%aimport credentials.cred\n",
    "from credentials import cred\n",
    "\n",
    "\n",
    "config = {\n",
    "  'host': cred.mongo_host,\n",
    "  'username': cred.mongo_user,\n",
    "  'password': cred.mongo_pass,\n",
    "  'authSource': cred.mongo_auth_db\n",
    "}\n",
    "\n",
    "# get a mongo client\n",
    "client = MongoClient(**config)\n",
    "\n",
    "# use the raw database\n",
    "jre_raw = client.jre_raw\n",
    "podcasts_raw = jre_raw.podcasts\n",
    "\n",
    "# use the clean database\n",
    "jre_clean = client.jre_clean\n",
    "podcasts_clean = jre_clean.podcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Podcasts from Podscribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "ps_ep_mongo = list(\n",
    "    podcasts_raw\n",
    "    .find({'source':'podscribe'},{'_id':0, 'text':1, 'name':1})\n",
    "    .limit(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out text and names\n",
    "ps_ep_list = [x['text'] for x in ps_ep_mongo]\n",
    "ps_ep_names = [x['name'] for x in ps_ep_mongo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_ep_list = [x.lower() for x in ps_ep_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "new_stop_words = []\n",
    "\n",
    "# profanity\n",
    "profanity = (\n",
    "    open('stop_words/profanity.txt', newline='\\n')\n",
    "    .read()\n",
    "    .splitlines()\n",
    ")\n",
    "\n",
    "new_stop_words += [x.lower() for x in profanity]\n",
    "\n",
    "# common Joe Rogan words\n",
    "common_jre_words = [\n",
    "    'like', 'yeah', 'know', 'just', 'right', 'right', 'think', 'know',\n",
    "    'people', 'going', 'really', 'got', 'thing', 'want', 'actually',\n",
    "    'say'\n",
    "]\n",
    "new_stop_words += common_jre_words\n",
    "\n",
    "# append to english stop words\n",
    "jre_stop_words = text.ENGLISH_STOP_WORDS.union(new_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i, ep in enumerate(ps_ep_list):\n",
    "    ps_ep_list[i] = ' '.join([lemmatizer.lemmatize(x) for x in ep.split(' ')])\n",
    "    \n",
    "jre_stop_words = [lemmatizer.lemmatize(x) for x in jre_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.8, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True,\n",
       "                stop_words=['assfukka', 'toward', 'very', 'fill', 'them',\n",
       "                            'hoer', 'ejaculated', 'all', 'bastard', 'wanker',\n",
       "                            'please', 'dogging', 'have', 'fanny', 'meanwhile',\n",
       "                            'latterly', 'during', 'either', 'shitfull',\n",
       "                            'twatty', 'moreover', 'everywhere', 'dink',\n",
       "                            'breast', 'of', 'pisser', 'per', 'fuckwhit', 'been',\n",
       "                            'fuckhead', ...],\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='\\\\b[a-z][a-z]+\\\\b', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "ps_ep_list_tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2), binary=True, stop_words=jre_stop_words,\n",
    "    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", max_df=0.8\n",
    ")\n",
    "\n",
    "# fit\n",
    "ps_ep_list_tfidf.fit(ps_ep_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 3), preprocessor=None,\n",
       "                stop_words=['assfukka', 'toward', 'very', 'fill', 'them',\n",
       "                            'hoer', 'ejaculated', 'all', 'bastard', 'wanker',\n",
       "                            'please', 'dogging', 'have', 'fanny', 'meanwhile',\n",
       "                            'latterly', 'during', 'either', 'shitfull',\n",
       "                            'twatty', 'moreover', 'everywhere', 'dink',\n",
       "                            'breast', 'of', 'pisser', 'per', 'fuckwhit', 'been',\n",
       "                            'fuckhead', ...],\n",
       "                strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer for parsing/counting words\n",
    "ps_ep_list_cvec = CountVectorizer(ngram_range=(1, 3),  \n",
    "                                   stop_words=jre_stop_words, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "# fit\n",
    "ps_ep_list_cvec.fit(ps_ep_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_ep_list_tokenizer = ps_ep_list_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# column names\n",
    "col_names = ['component_'+str(x) for x in range(1,num_topics+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "doc_word = ps_ep_list_tokenizer.transform(ps_ep_list).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa charge</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa couple</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa cross</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa denver</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1    2    3    4    5    6    7         8    9   ...   90  \\\n",
       "aa         0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.006962  0.0  ...  0.0   \n",
       "aa charge  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.010651  0.0  ...  0.0   \n",
       "aa couple  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
       "aa cross   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
       "aa denver  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
       "\n",
       "            91        92   93   94   95        96   97   98   99  \n",
       "aa         0.0  0.006068  0.0  0.0  0.0  0.006922  0.0  0.0  0.0  \n",
       "aa charge  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  \n",
       "aa couple  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  \n",
       "aa cross   0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  \n",
       "aa denver  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(doc_word.toarray(), ps_ep_list_tokenizer.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in ps_ep_list_tokenizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 19:33:21,272 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2020-02-19 19:33:21,274 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2020-02-19 19:33:21,384 : INFO : using serial LDA version on this node\n",
      "2020-02-19 19:33:21,620 : INFO : running online (multi-pass) LDA training, 3 topics, 5 passes over the supplied corpus of 100 documents, updating model once every 100 documents, evaluating perplexity every 100 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-02-19 19:33:21,621 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2020-02-19 19:33:28,988 : INFO : -95.322 per-word bound, 49505244778645621142204710912.0 perplexity estimate based on a held-out corpus of 100 documents with 9759 words\n",
      "2020-02-19 19:33:28,989 : INFO : PROGRESS: pass 0, at document #100/100\n",
      "2020-02-19 19:33:30,670 : INFO : topic #0 (0.333): 0.000*\"paying\" + 0.000*\"fantastic\" + 0.000*\"jesus christ\" + 0.000*\"plane\" + 0.000*\"shouldn\" + 0.000*\"news\" + 0.000*\"worried\" + 0.000*\"aspect\" + 0.000*\"mom\" + 0.000*\"fly\"\n",
      "2020-02-19 19:33:30,682 : INFO : topic #1 (0.333): 0.000*\"career\" + 0.000*\"la\" + 0.000*\"wear\" + 0.000*\"horrible\" + 0.000*\"guy come\" + 0.000*\"hard time\" + 0.000*\"apart\" + 0.000*\"tool\" + 0.000*\"asking\" + 0.000*\"notice\"\n",
      "2020-02-19 19:33:30,692 : INFO : topic #2 (0.333): 0.000*\"risk\" + 0.000*\"higher\" + 0.000*\"waiting\" + 0.000*\"rare\" + 0.000*\"create\" + 0.000*\"speaking\" + 0.000*\"yep\" + 0.000*\"did did\" + 0.000*\"window\" + 0.000*\"available\"\n",
      "2020-02-19 19:33:30,697 : INFO : topic diff=1.579587, rho=1.000000\n",
      "2020-02-19 19:33:38,186 : INFO : -15.377 per-word bound, 42568.4 perplexity estimate based on a held-out corpus of 100 documents with 9759 words\n",
      "2020-02-19 19:33:38,186 : INFO : PROGRESS: pass 1, at document #100/100\n",
      "2020-02-19 19:33:39,866 : INFO : topic #0 (0.333): 0.000*\"fantastic\" + 0.000*\"paying\" + 0.000*\"shouldn\" + 0.000*\"jesus christ\" + 0.000*\"aspect\" + 0.000*\"plane\" + 0.000*\"mom\" + 0.000*\"news\" + 0.000*\"energy\" + 0.000*\"fly\"\n",
      "2020-02-19 19:33:39,878 : INFO : topic #1 (0.333): 0.000*\"career\" + 0.000*\"la\" + 0.000*\"horrible\" + 0.000*\"wear\" + 0.000*\"asking\" + 0.000*\"exact\" + 0.000*\"apart\" + 0.000*\"tool\" + 0.000*\"guy come\" + 0.000*\"notice\"\n",
      "2020-02-19 19:33:39,889 : INFO : topic #2 (0.333): 0.000*\"higher\" + 0.000*\"risk\" + 0.000*\"waiting\" + 0.000*\"rare\" + 0.000*\"create\" + 0.000*\"did did\" + 0.000*\"yep\" + 0.000*\"time time\" + 0.000*\"decision\" + 0.000*\"window\"\n",
      "2020-02-19 19:33:39,893 : INFO : topic diff=0.004345, rho=0.577350\n",
      "2020-02-19 19:33:47,389 : INFO : -15.378 per-word bound, 42577.4 perplexity estimate based on a held-out corpus of 100 documents with 9759 words\n",
      "2020-02-19 19:33:47,390 : INFO : PROGRESS: pass 2, at document #100/100\n",
      "2020-02-19 19:33:49,089 : INFO : topic #0 (0.333): 0.000*\"fantastic\" + 0.000*\"paying\" + 0.000*\"aspect\" + 0.000*\"shouldn\" + 0.000*\"jesus christ\" + 0.000*\"mom\" + 0.000*\"energy\" + 0.000*\"fly\" + 0.000*\"don feel\" + 0.000*\"news\"\n",
      "2020-02-19 19:33:49,101 : INFO : topic #1 (0.333): 0.000*\"career\" + 0.000*\"horrible\" + 0.000*\"exact\" + 0.000*\"la\" + 0.000*\"asking\" + 0.000*\"gigantic\" + 0.000*\"apart\" + 0.000*\"notice\" + 0.000*\"download cash\" + 0.000*\"designed\"\n",
      "2020-02-19 19:33:49,110 : INFO : topic #2 (0.333): 0.000*\"higher\" + 0.000*\"rare\" + 0.000*\"did did\" + 0.000*\"waiting\" + 0.000*\"look look\" + 0.000*\"time time\" + 0.000*\"risk\" + 0.000*\"decision\" + 0.000*\"create\" + 0.000*\"available\"\n",
      "2020-02-19 19:33:49,114 : INFO : topic diff=0.002659, rho=0.500000\n",
      "2020-02-19 19:33:56,633 : INFO : -15.377 per-word bound, 42560.0 perplexity estimate based on a held-out corpus of 100 documents with 9759 words\n",
      "2020-02-19 19:33:56,634 : INFO : PROGRESS: pass 3, at document #100/100\n",
      "2020-02-19 19:33:58,289 : INFO : topic #0 (0.333): 0.000*\"fantastic\" + 0.000*\"aspect\" + 0.000*\"mom\" + 0.000*\"energy\" + 0.000*\"shouldn\" + 0.000*\"paying\" + 0.000*\"don feel\" + 0.000*\"fly\" + 0.000*\"essentially\" + 0.000*\"jesus christ\"\n",
      "2020-02-19 19:33:58,299 : INFO : topic #1 (0.333): 0.000*\"career\" + 0.000*\"horrible\" + 0.000*\"exact\" + 0.000*\"asking\" + 0.000*\"designed\" + 0.000*\"la\" + 0.000*\"gigantic\" + 0.000*\"ran\" + 0.000*\"notice\" + 0.000*\"healthy\"\n",
      "2020-02-19 19:33:58,306 : INFO : topic #2 (0.333): 0.000*\"rare\" + 0.000*\"look look\" + 0.000*\"did did\" + 0.000*\"comedy\" + 0.000*\"drunk\" + 0.000*\"time time\" + 0.000*\"decision\" + 0.000*\"higher\" + 0.000*\"boom\" + 0.000*\"waiting\"\n",
      "2020-02-19 19:33:58,310 : INFO : topic diff=0.002343, rho=0.447214\n",
      "2020-02-19 19:34:05,756 : INFO : -15.376 per-word bound, 42513.4 perplexity estimate based on a held-out corpus of 100 documents with 9759 words\n",
      "2020-02-19 19:34:05,756 : INFO : PROGRESS: pass 4, at document #100/100\n",
      "2020-02-19 19:34:07,463 : INFO : topic #0 (0.333): 0.000*\"aspect\" + 0.000*\"mom\" + 0.000*\"fantastic\" + 0.000*\"essentially\" + 0.000*\"don feel\" + 0.000*\"welcome\" + 0.000*\"energy\" + 0.000*\"standing\" + 0.000*\"fly\" + 0.000*\"computer\"\n",
      "2020-02-19 19:34:07,473 : INFO : topic #1 (0.333): 0.000*\"horrible\" + 0.000*\"career\" + 0.000*\"exact\" + 0.000*\"designed\" + 0.000*\"asking\" + 0.000*\"gigantic\" + 0.000*\"written\" + 0.000*\"ran\" + 0.000*\"healthy\" + 0.000*\"television\"\n",
      "2020-02-19 19:34:07,481 : INFO : topic #2 (0.333): 0.000*\"rare\" + 0.000*\"look look\" + 0.000*\"comedy\" + 0.000*\"did did\" + 0.000*\"drunk\" + 0.000*\"beat\" + 0.000*\"time time\" + 0.000*\"expensive\" + 0.000*\"blow\" + 0.000*\"goal\"\n",
      "2020-02-19 19:34:07,485 : INFO : topic diff=0.002263, rho=0.408248\n"
     ]
    }
   ],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 19:34:07,696 : INFO : topic #0 (0.333): 0.000*\"aspect\" + 0.000*\"mom\" + 0.000*\"fantastic\" + 0.000*\"essentially\" + 0.000*\"don feel\" + 0.000*\"welcome\" + 0.000*\"energy\" + 0.000*\"standing\" + 0.000*\"fly\" + 0.000*\"computer\"\n",
      "2020-02-19 19:34:07,709 : INFO : topic #1 (0.333): 0.000*\"horrible\" + 0.000*\"career\" + 0.000*\"exact\" + 0.000*\"designed\" + 0.000*\"asking\" + 0.000*\"gigantic\" + 0.000*\"written\" + 0.000*\"ran\" + 0.000*\"healthy\" + 0.000*\"television\"\n",
      "2020-02-19 19:34:07,718 : INFO : topic #2 (0.333): 0.000*\"rare\" + 0.000*\"look look\" + 0.000*\"comedy\" + 0.000*\"did did\" + 0.000*\"drunk\" + 0.000*\"beat\" + 0.000*\"time time\" + 0.000*\"expensive\" + 0.000*\"blow\" + 0.000*\"goal\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*\"aspect\" + 0.000*\"mom\" + 0.000*\"fantastic\" + 0.000*\"essentially\" + 0.000*\"don feel\" + 0.000*\"welcome\" + 0.000*\"energy\" + 0.000*\"standing\" + 0.000*\"fly\" + 0.000*\"computer\"'),\n",
       " (1,\n",
       "  '0.000*\"horrible\" + 0.000*\"career\" + 0.000*\"exact\" + 0.000*\"designed\" + 0.000*\"asking\" + 0.000*\"gigantic\" + 0.000*\"written\" + 0.000*\"ran\" + 0.000*\"healthy\" + 0.000*\"television\"'),\n",
       " (2,\n",
       "  '0.000*\"rare\" + 0.000*\"look look\" + 0.000*\"comedy\" + 0.000*\"did did\" + 0.000*\"drunk\" + 0.000*\"beat\" + 0.000*\"time time\" + 0.000*\"expensive\" + 0.000*\"blow\" + 0.000*\"goal\"')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 618792)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word = ps_ep_list_tokenizer.transform(ps_ep_list)\n",
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa charge</th>\n",
       "      <th>aa couple</th>\n",
       "      <th>aa cross</th>\n",
       "      <th>aa denver</th>\n",
       "      <th>aa judah</th>\n",
       "      <th>aa look</th>\n",
       "      <th>aa meeting</th>\n",
       "      <th>aa palace</th>\n",
       "      <th>aa steve</th>\n",
       "      <th>...</th>\n",
       "      <th>zygote different</th>\n",
       "      <th>zyklon</th>\n",
       "      <th>zyklon gas</th>\n",
       "      <th>zyklon remove</th>\n",
       "      <th>zz</th>\n",
       "      <th>zz coffee</th>\n",
       "      <th>zz coming</th>\n",
       "      <th>zz decided</th>\n",
       "      <th>zz funk</th>\n",
       "      <th>zz riff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#1383 - Malcolm Gladwell</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1382 - RZA &amp; Donnell Rawlings</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1378 - Greg Fitzsimmons</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1375 - Edward Norton</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1373 - Kyle Kulinski</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1371 - Andrew Santino</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1370 - Brian Grazer</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1369 - Christopher Ryan</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1368 - Edward Snowden</th>\n",
       "      <td>0.006962</td>\n",
       "      <td>0.010651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1366 - Richard Dawkins</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 618792 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       aa  aa charge  aa couple  aa cross  \\\n",
       " #1383 - Malcolm Gladwell        0.000000   0.000000        0.0       0.0   \n",
       " #1382 - RZA & Donnell Rawlings  0.000000   0.000000        0.0       0.0   \n",
       " #1378 - Greg Fitzsimmons        0.000000   0.000000        0.0       0.0   \n",
       " #1375 - Edward Norton           0.000000   0.000000        0.0       0.0   \n",
       " #1373 - Kyle Kulinski           0.000000   0.000000        0.0       0.0   \n",
       " #1371 - Andrew Santino          0.000000   0.000000        0.0       0.0   \n",
       " #1370 - Brian Grazer            0.000000   0.000000        0.0       0.0   \n",
       " #1369 - Christopher Ryan        0.000000   0.000000        0.0       0.0   \n",
       " #1368 - Edward Snowden          0.006962   0.010651        0.0       0.0   \n",
       " #1366 - Richard Dawkins         0.000000   0.000000        0.0       0.0   \n",
       "\n",
       "                                 aa denver  aa judah  aa look  aa meeting  \\\n",
       " #1383 - Malcolm Gladwell              0.0       0.0      0.0         0.0   \n",
       " #1382 - RZA & Donnell Rawlings        0.0       0.0      0.0         0.0   \n",
       " #1378 - Greg Fitzsimmons              0.0       0.0      0.0         0.0   \n",
       " #1375 - Edward Norton                 0.0       0.0      0.0         0.0   \n",
       " #1373 - Kyle Kulinski                 0.0       0.0      0.0         0.0   \n",
       " #1371 - Andrew Santino                0.0       0.0      0.0         0.0   \n",
       " #1370 - Brian Grazer                  0.0       0.0      0.0         0.0   \n",
       " #1369 - Christopher Ryan              0.0       0.0      0.0         0.0   \n",
       " #1368 - Edward Snowden                0.0       0.0      0.0         0.0   \n",
       " #1366 - Richard Dawkins               0.0       0.0      0.0         0.0   \n",
       "\n",
       "                                 aa palace  aa steve  ...  zygote different  \\\n",
       " #1383 - Malcolm Gladwell              0.0       0.0  ...               0.0   \n",
       " #1382 - RZA & Donnell Rawlings        0.0       0.0  ...               0.0   \n",
       " #1378 - Greg Fitzsimmons              0.0       0.0  ...               0.0   \n",
       " #1375 - Edward Norton                 0.0       0.0  ...               0.0   \n",
       " #1373 - Kyle Kulinski                 0.0       0.0  ...               0.0   \n",
       " #1371 - Andrew Santino                0.0       0.0  ...               0.0   \n",
       " #1370 - Brian Grazer                  0.0       0.0  ...               0.0   \n",
       " #1369 - Christopher Ryan              0.0       0.0  ...               0.0   \n",
       " #1368 - Edward Snowden                0.0       0.0  ...               0.0   \n",
       " #1366 - Richard Dawkins               0.0       0.0  ...               0.0   \n",
       "\n",
       "                                 zyklon  zyklon gas  zyklon remove   zz  \\\n",
       " #1383 - Malcolm Gladwell           0.0         0.0            0.0  0.0   \n",
       " #1382 - RZA & Donnell Rawlings     0.0         0.0            0.0  0.0   \n",
       " #1378 - Greg Fitzsimmons           0.0         0.0            0.0  0.0   \n",
       " #1375 - Edward Norton              0.0         0.0            0.0  0.0   \n",
       " #1373 - Kyle Kulinski              0.0         0.0            0.0  0.0   \n",
       " #1371 - Andrew Santino             0.0         0.0            0.0  0.0   \n",
       " #1370 - Brian Grazer               0.0         0.0            0.0  0.0   \n",
       " #1369 - Christopher Ryan           0.0         0.0            0.0  0.0   \n",
       " #1368 - Edward Snowden             0.0         0.0            0.0  0.0   \n",
       " #1366 - Richard Dawkins            0.0         0.0            0.0  0.0   \n",
       "\n",
       "                                 zz coffee  zz coming  zz decided  zz funk  \\\n",
       " #1383 - Malcolm Gladwell              0.0        0.0         0.0      0.0   \n",
       " #1382 - RZA & Donnell Rawlings        0.0        0.0         0.0      0.0   \n",
       " #1378 - Greg Fitzsimmons              0.0        0.0         0.0      0.0   \n",
       " #1375 - Edward Norton                 0.0        0.0         0.0      0.0   \n",
       " #1373 - Kyle Kulinski                 0.0        0.0         0.0      0.0   \n",
       " #1371 - Andrew Santino                0.0        0.0         0.0      0.0   \n",
       " #1370 - Brian Grazer                  0.0        0.0         0.0      0.0   \n",
       " #1369 - Christopher Ryan              0.0        0.0         0.0      0.0   \n",
       " #1368 - Edward Snowden                0.0        0.0         0.0      0.0   \n",
       " #1366 - Richard Dawkins               0.0        0.0         0.0      0.0   \n",
       "\n",
       "                                 zz riff  \n",
       " #1383 - Malcolm Gladwell            0.0  \n",
       " #1382 - RZA & Donnell Rawlings      0.0  \n",
       " #1378 - Greg Fitzsimmons            0.0  \n",
       " #1375 - Edward Norton               0.0  \n",
       " #1373 - Kyle Kulinski               0.0  \n",
       " #1371 - Andrew Santino              0.0  \n",
       " #1370 - Brian Grazer                0.0  \n",
       " #1369 - Christopher Ryan            0.0  \n",
       " #1368 - Edward Snowden              0.0  \n",
       " #1366 - Richard Dawkins             0.0  \n",
       "\n",
       "[10 rows x 618792 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have to convert `.toarray()` because the vectorizer returns a sparse matrix.\n",
    "# For a big corpus, we would skip the dataframe and keep the output sparse.\n",
    "pd.DataFrame(doc_word.toarray(), index=ps_ep_names, columns=ps_ep_list_tokenizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00072207, 0.01119541, 0.01044549, 0.01041946, 0.01036292])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "#  Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "lsa = TruncatedSVD(num_topics)\n",
    "doc_topic = lsa.fit_transform(doc_word)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa charge</th>\n",
       "      <th>aa couple</th>\n",
       "      <th>aa cross</th>\n",
       "      <th>aa denver</th>\n",
       "      <th>aa judah</th>\n",
       "      <th>aa look</th>\n",
       "      <th>aa meeting</th>\n",
       "      <th>aa palace</th>\n",
       "      <th>aa steve</th>\n",
       "      <th>...</th>\n",
       "      <th>zygote different</th>\n",
       "      <th>zyklon</th>\n",
       "      <th>zyklon gas</th>\n",
       "      <th>zyklon remove</th>\n",
       "      <th>zz</th>\n",
       "      <th>zz coffee</th>\n",
       "      <th>zz coming</th>\n",
       "      <th>zz decided</th>\n",
       "      <th>zz funk</th>\n",
       "      <th>zz riff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_5</th>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 618792 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                aa  aa charge  aa couple  aa cross  aa denver  aa judah  \\\n",
       "component_1  0.004      0.001      0.001     0.001      0.001     0.001   \n",
       "component_2  0.001      0.003      0.000     0.001     -0.001     0.001   \n",
       "component_3 -0.006     -0.001     -0.001    -0.001     -0.004    -0.001   \n",
       "component_4  0.002     -0.000      0.002     0.000      0.000     0.000   \n",
       "component_5 -0.004     -0.000     -0.001    -0.001     -0.002    -0.002   \n",
       "\n",
       "             aa look  aa meeting  aa palace  aa steve  ...  zygote different  \\\n",
       "component_1    0.001       0.001      0.001     0.001  ...             0.001   \n",
       "component_2    0.000      -0.002     -0.001     0.000  ...             0.002   \n",
       "component_3   -0.001      -0.000     -0.001     0.000  ...            -0.000   \n",
       "component_4    0.001       0.000      0.002    -0.001  ...             0.003   \n",
       "component_5    0.000      -0.000      0.000    -0.001  ...            -0.000   \n",
       "\n",
       "             zyklon  zyklon gas  zyklon remove     zz  zz coffee  zz coming  \\\n",
       "component_1   0.001       0.001          0.001  0.002      0.001      0.001   \n",
       "component_2   0.003       0.003          0.003 -0.001     -0.001      0.000   \n",
       "component_3   0.000       0.000          0.000  0.000     -0.000     -0.000   \n",
       "component_4   0.001       0.001          0.001 -0.003     -0.000      0.000   \n",
       "component_5  -0.002      -0.002         -0.002  0.000     -0.001      0.004   \n",
       "\n",
       "             zz decided  zz funk  zz riff  \n",
       "component_1       0.001    0.001    0.001  \n",
       "component_2      -0.001    0.000   -0.001  \n",
       "component_3      -0.000    0.001   -0.000  \n",
       "component_4      -0.003   -0.001   -0.003  \n",
       "component_5      -0.000   -0.002   -0.000  \n",
       "\n",
       "[5 rows x 618792 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = col_names,\n",
    "             columns = ps_ep_list_tokenizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "comedy, fat, apparently, club, growing\n",
      "\n",
      "Topic  1\n",
      "economic, revolution, discussing, elected, justification\n",
      "\n",
      "Topic  2\n",
      "year way, volunteer, chief, welcoming, happens die\n",
      "\n",
      "Topic  3\n",
      "com enter, jre, seven thousand, month free, postage\n",
      "\n",
      "Topic  4\n",
      "strain, thought sort, pretty heavy, stunt, lenny bruce\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, ps_ep_list_tokenizer.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "      <th>component_3</th>\n",
       "      <th>component_4</th>\n",
       "      <th>component_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#1383 - Malcolm Gladwell</th>\n",
       "      <td>0.18865</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>-0.14375</td>\n",
       "      <td>-0.01392</td>\n",
       "      <td>0.10970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1382 - RZA &amp; Donnell Rawlings</th>\n",
       "      <td>0.17352</td>\n",
       "      <td>0.01368</td>\n",
       "      <td>-0.07292</td>\n",
       "      <td>0.00202</td>\n",
       "      <td>-0.01293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1378 - Greg Fitzsimmons</th>\n",
       "      <td>0.19821</td>\n",
       "      <td>0.02892</td>\n",
       "      <td>-0.09059</td>\n",
       "      <td>-0.07588</td>\n",
       "      <td>-0.17435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1375 - Edward Norton</th>\n",
       "      <td>0.14573</td>\n",
       "      <td>0.01722</td>\n",
       "      <td>0.01402</td>\n",
       "      <td>0.08765</td>\n",
       "      <td>0.24600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1373 - Kyle Kulinski</th>\n",
       "      <td>0.18779</td>\n",
       "      <td>0.15481</td>\n",
       "      <td>-0.06647</td>\n",
       "      <td>-0.03001</td>\n",
       "      <td>-0.05193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1280 - Michael Yo</th>\n",
       "      <td>0.21437</td>\n",
       "      <td>-0.13083</td>\n",
       "      <td>-0.09648</td>\n",
       "      <td>-0.05392</td>\n",
       "      <td>-0.05380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1279 - Jessimae Peluso</th>\n",
       "      <td>0.18865</td>\n",
       "      <td>0.05627</td>\n",
       "      <td>-0.07005</td>\n",
       "      <td>0.01533</td>\n",
       "      <td>-0.16422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1278 - Kevin Hart</th>\n",
       "      <td>0.16575</td>\n",
       "      <td>-0.03266</td>\n",
       "      <td>0.01238</td>\n",
       "      <td>0.13365</td>\n",
       "      <td>-0.03518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1277 - Gabrielle Reece</th>\n",
       "      <td>0.17820</td>\n",
       "      <td>-0.10176</td>\n",
       "      <td>0.03490</td>\n",
       "      <td>-0.15709</td>\n",
       "      <td>-0.09084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1276 - Ben Shapiro</th>\n",
       "      <td>0.16550</td>\n",
       "      <td>0.01435</td>\n",
       "      <td>-0.02499</td>\n",
       "      <td>0.05966</td>\n",
       "      <td>-0.10902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 component_1  component_2  component_3  \\\n",
       " #1383 - Malcolm Gladwell            0.18865      0.11700     -0.14375   \n",
       " #1382 - RZA & Donnell Rawlings      0.17352      0.01368     -0.07292   \n",
       " #1378 - Greg Fitzsimmons            0.19821      0.02892     -0.09059   \n",
       " #1375 - Edward Norton               0.14573      0.01722      0.01402   \n",
       " #1373 - Kyle Kulinski               0.18779      0.15481     -0.06647   \n",
       "...                                      ...          ...          ...   \n",
       " #1280 - Michael Yo                  0.21437     -0.13083     -0.09648   \n",
       " #1279 - Jessimae Peluso             0.18865      0.05627     -0.07005   \n",
       " #1278 - Kevin Hart                  0.16575     -0.03266      0.01238   \n",
       " #1277 - Gabrielle Reece             0.17820     -0.10176      0.03490   \n",
       " #1276 - Ben Shapiro                 0.16550      0.01435     -0.02499   \n",
       "\n",
       "                                 component_4  component_5  \n",
       " #1383 - Malcolm Gladwell           -0.01392      0.10970  \n",
       " #1382 - RZA & Donnell Rawlings      0.00202     -0.01293  \n",
       " #1378 - Greg Fitzsimmons           -0.07588     -0.17435  \n",
       " #1375 - Edward Norton               0.08765      0.24600  \n",
       " #1373 - Kyle Kulinski              -0.03001     -0.05193  \n",
       "...                                      ...          ...  \n",
       " #1280 - Michael Yo                 -0.05392     -0.05380  \n",
       " #1279 - Jessimae Peluso             0.01533     -0.16422  \n",
       " #1278 - Kevin Hart                  0.13365     -0.03518  \n",
       " #1277 - Gabrielle Reece            -0.15709     -0.09084  \n",
       " #1276 - Ben Shapiro                 0.05966     -0.10902  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vt = pd.DataFrame(doc_topic.round(5),\n",
    "             index = ps_ep_names,\n",
    "             columns = col_names)\n",
    "Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa charge</th>\n",
       "      <th>aa couple</th>\n",
       "      <th>aa cross</th>\n",
       "      <th>aa denver</th>\n",
       "      <th>aa judah</th>\n",
       "      <th>aa look</th>\n",
       "      <th>aa meeting</th>\n",
       "      <th>aa palace</th>\n",
       "      <th>aa steve</th>\n",
       "      <th>...</th>\n",
       "      <th>zygote different</th>\n",
       "      <th>zyklon</th>\n",
       "      <th>zyklon gas</th>\n",
       "      <th>zyklon remove</th>\n",
       "      <th>zz</th>\n",
       "      <th>zz coffee</th>\n",
       "      <th>zz coming</th>\n",
       "      <th>zz decided</th>\n",
       "      <th>zz funk</th>\n",
       "      <th>zz riff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#1383 - Malcolm Gladwell</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1382 - RZA &amp; Donnell Rawlings</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1378 - Greg Fitzsimmons</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1375 - Edward Norton</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1373 - Kyle Kulinski</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1371 - Andrew Santino</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1370 - Brian Grazer</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1369 - Christopher Ryan</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1368 - Edward Snowden</th>\n",
       "      <td>0.006962</td>\n",
       "      <td>0.010651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1366 - Richard Dawkins</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 618792 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       aa  aa charge  aa couple  aa cross  \\\n",
       " #1383 - Malcolm Gladwell        0.000000   0.000000        0.0       0.0   \n",
       " #1382 - RZA & Donnell Rawlings  0.000000   0.000000        0.0       0.0   \n",
       " #1378 - Greg Fitzsimmons        0.000000   0.000000        0.0       0.0   \n",
       " #1375 - Edward Norton           0.000000   0.000000        0.0       0.0   \n",
       " #1373 - Kyle Kulinski           0.000000   0.000000        0.0       0.0   \n",
       " #1371 - Andrew Santino          0.000000   0.000000        0.0       0.0   \n",
       " #1370 - Brian Grazer            0.000000   0.000000        0.0       0.0   \n",
       " #1369 - Christopher Ryan        0.000000   0.000000        0.0       0.0   \n",
       " #1368 - Edward Snowden          0.006962   0.010651        0.0       0.0   \n",
       " #1366 - Richard Dawkins         0.000000   0.000000        0.0       0.0   \n",
       "\n",
       "                                 aa denver  aa judah  aa look  aa meeting  \\\n",
       " #1383 - Malcolm Gladwell              0.0       0.0      0.0         0.0   \n",
       " #1382 - RZA & Donnell Rawlings        0.0       0.0      0.0         0.0   \n",
       " #1378 - Greg Fitzsimmons              0.0       0.0      0.0         0.0   \n",
       " #1375 - Edward Norton                 0.0       0.0      0.0         0.0   \n",
       " #1373 - Kyle Kulinski                 0.0       0.0      0.0         0.0   \n",
       " #1371 - Andrew Santino                0.0       0.0      0.0         0.0   \n",
       " #1370 - Brian Grazer                  0.0       0.0      0.0         0.0   \n",
       " #1369 - Christopher Ryan              0.0       0.0      0.0         0.0   \n",
       " #1368 - Edward Snowden                0.0       0.0      0.0         0.0   \n",
       " #1366 - Richard Dawkins               0.0       0.0      0.0         0.0   \n",
       "\n",
       "                                 aa palace  aa steve  ...  zygote different  \\\n",
       " #1383 - Malcolm Gladwell              0.0       0.0  ...               0.0   \n",
       " #1382 - RZA & Donnell Rawlings        0.0       0.0  ...               0.0   \n",
       " #1378 - Greg Fitzsimmons              0.0       0.0  ...               0.0   \n",
       " #1375 - Edward Norton                 0.0       0.0  ...               0.0   \n",
       " #1373 - Kyle Kulinski                 0.0       0.0  ...               0.0   \n",
       " #1371 - Andrew Santino                0.0       0.0  ...               0.0   \n",
       " #1370 - Brian Grazer                  0.0       0.0  ...               0.0   \n",
       " #1369 - Christopher Ryan              0.0       0.0  ...               0.0   \n",
       " #1368 - Edward Snowden                0.0       0.0  ...               0.0   \n",
       " #1366 - Richard Dawkins               0.0       0.0  ...               0.0   \n",
       "\n",
       "                                 zyklon  zyklon gas  zyklon remove   zz  \\\n",
       " #1383 - Malcolm Gladwell           0.0         0.0            0.0  0.0   \n",
       " #1382 - RZA & Donnell Rawlings     0.0         0.0            0.0  0.0   \n",
       " #1378 - Greg Fitzsimmons           0.0         0.0            0.0  0.0   \n",
       " #1375 - Edward Norton              0.0         0.0            0.0  0.0   \n",
       " #1373 - Kyle Kulinski              0.0         0.0            0.0  0.0   \n",
       " #1371 - Andrew Santino             0.0         0.0            0.0  0.0   \n",
       " #1370 - Brian Grazer               0.0         0.0            0.0  0.0   \n",
       " #1369 - Christopher Ryan           0.0         0.0            0.0  0.0   \n",
       " #1368 - Edward Snowden             0.0         0.0            0.0  0.0   \n",
       " #1366 - Richard Dawkins            0.0         0.0            0.0  0.0   \n",
       "\n",
       "                                 zz coffee  zz coming  zz decided  zz funk  \\\n",
       " #1383 - Malcolm Gladwell              0.0        0.0         0.0      0.0   \n",
       " #1382 - RZA & Donnell Rawlings        0.0        0.0         0.0      0.0   \n",
       " #1378 - Greg Fitzsimmons              0.0        0.0         0.0      0.0   \n",
       " #1375 - Edward Norton                 0.0        0.0         0.0      0.0   \n",
       " #1373 - Kyle Kulinski                 0.0        0.0         0.0      0.0   \n",
       " #1371 - Andrew Santino                0.0        0.0         0.0      0.0   \n",
       " #1370 - Brian Grazer                  0.0        0.0         0.0      0.0   \n",
       " #1369 - Christopher Ryan              0.0        0.0         0.0      0.0   \n",
       " #1368 - Edward Snowden                0.0        0.0         0.0      0.0   \n",
       " #1366 - Richard Dawkins               0.0        0.0         0.0      0.0   \n",
       "\n",
       "                                 zz riff  \n",
       " #1383 - Malcolm Gladwell            0.0  \n",
       " #1382 - RZA & Donnell Rawlings      0.0  \n",
       " #1378 - Greg Fitzsimmons            0.0  \n",
       " #1375 - Edward Norton               0.0  \n",
       " #1373 - Kyle Kulinski               0.0  \n",
       " #1371 - Andrew Santino              0.0  \n",
       " #1370 - Brian Grazer                0.0  \n",
       " #1369 - Christopher Ryan            0.0  \n",
       " #1368 - Edward Snowden              0.0  \n",
       " #1366 - Richard Dawkins             0.0  \n",
       "\n",
       "[10 rows x 618792 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word = ps_ep_list_tokenizer.transform(ps_ep_list)\n",
    "pd.DataFrame(doc_word.toarray(), index=ps_ep_names, columns=ps_ep_list_tokenizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(num_topics)\n",
    "doc_topic = nmf_model.fit_transform(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa charge</th>\n",
       "      <th>aa couple</th>\n",
       "      <th>aa cross</th>\n",
       "      <th>aa denver</th>\n",
       "      <th>aa judah</th>\n",
       "      <th>aa look</th>\n",
       "      <th>aa meeting</th>\n",
       "      <th>aa palace</th>\n",
       "      <th>aa steve</th>\n",
       "      <th>...</th>\n",
       "      <th>zygote different</th>\n",
       "      <th>zyklon</th>\n",
       "      <th>zyklon gas</th>\n",
       "      <th>zyklon remove</th>\n",
       "      <th>zz</th>\n",
       "      <th>zz coffee</th>\n",
       "      <th>zz coming</th>\n",
       "      <th>zz decided</th>\n",
       "      <th>zz funk</th>\n",
       "      <th>zz riff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_5</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 618792 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                aa  aa charge  aa couple  aa cross  aa denver  aa judah  \\\n",
       "component_1  0.005      0.000      0.000     0.000      0.002     0.002   \n",
       "component_2  0.003      0.002      0.000     0.002      0.000     0.000   \n",
       "component_3  0.005      0.000      0.000     0.000      0.000     0.000   \n",
       "component_4  0.000      0.000      0.000     0.000      0.000     0.000   \n",
       "component_5  0.002      0.000      0.003     0.000      0.000     0.000   \n",
       "\n",
       "             aa look  aa meeting  aa palace  aa steve  ...  zygote different  \\\n",
       "component_1    0.000       0.003      0.000     0.000  ...             0.000   \n",
       "component_2    0.001       0.000      0.000     0.000  ...             0.002   \n",
       "component_3    0.000       0.000      0.006     0.002  ...             0.000   \n",
       "component_4    0.000       0.000      0.000     0.001  ...             0.000   \n",
       "component_5    0.000       0.000      0.000     0.000  ...             0.000   \n",
       "\n",
       "             zyklon  zyklon gas  zyklon remove     zz  zz coffee  zz coming  \\\n",
       "component_1   0.000       0.000          0.000  0.004      0.002      0.002   \n",
       "component_2   0.002       0.002          0.002  0.000      0.000      0.000   \n",
       "component_3   0.000       0.000          0.000  0.000      0.000      0.000   \n",
       "component_4   0.000       0.000          0.000  0.004      0.000      0.000   \n",
       "component_5   0.000       0.000          0.000  0.000      0.000      0.000   \n",
       "\n",
       "             zz decided  zz funk  zz riff  \n",
       "component_1       0.001    0.000    0.001  \n",
       "component_2       0.000    0.000    0.000  \n",
       "component_3       0.000    0.000    0.000  \n",
       "component_4       0.001    0.004    0.001  \n",
       "component_5       0.000    0.000    0.000  \n",
       "\n",
       "[5 rows x 618792 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = col_names,\n",
    "             columns = ps_ep_list_tokenizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "oh good, bro, jesus christ, weekend, bar, oh oh, comic, dance, song, laughing\n",
      "\n",
      "Topic  1\n",
      "economic, revolution, discussing, discussion, complex, result, political, deeply, bias, entity\n",
      "\n",
      "Topic  2\n",
      "fun learn, long run, help body, trying catch, soy, hone ranch, jet lag, shot hit, ounce, remember doing\n",
      "\n",
      "Topic  3\n",
      "fan love, deliver, make don, mini, movie said, prep, able buy, accident, love death, best music\n",
      "\n",
      "Topic  4\n",
      "green athletic, maintaining zero, proven vitamin, food sourced, sourced ingredient, dairy gluten, compromise approach, approach formulation, jerry, mineral alkaline\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, ps_ep_list_tokenizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "      <th>component_3</th>\n",
       "      <th>component_4</th>\n",
       "      <th>component_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#1383 - Malcolm Gladwell</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00780</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.37409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1382 - RZA &amp; Donnell Rawlings</th>\n",
       "      <td>0.12964</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00077</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1378 - Greg Fitzsimmons</th>\n",
       "      <td>0.14801</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1375 - Edward Norton</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.34823</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1373 - Kyle Kulinski</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.28154</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1280 - Michael Yo</th>\n",
       "      <td>0.16026</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1279 - Jessimae Peluso</th>\n",
       "      <td>0.14076</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1278 - Kevin Hart</th>\n",
       "      <td>0.11884</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.01037</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1277 - Gabrielle Reece</th>\n",
       "      <td>0.12883</td>\n",
       "      <td>0.00420</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1276 - Ben Shapiro</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.25213</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 component_1  component_2  component_3  \\\n",
       " #1383 - Malcolm Gladwell            0.00000      0.00780      0.00000   \n",
       " #1382 - RZA & Donnell Rawlings      0.12964      0.00000      0.00000   \n",
       " #1378 - Greg Fitzsimmons            0.14801      0.00000      0.00000   \n",
       " #1375 - Edward Norton               0.00000      0.00000      0.00000   \n",
       " #1373 - Kyle Kulinski               0.00000      0.28154      0.00000   \n",
       "...                                      ...          ...          ...   \n",
       " #1280 - Michael Yo                  0.16026      0.00000      0.00000   \n",
       " #1279 - Jessimae Peluso             0.14076      0.00000      0.00000   \n",
       " #1278 - Kevin Hart                  0.11884      0.00006      0.01037   \n",
       " #1277 - Gabrielle Reece             0.12883      0.00420      0.00000   \n",
       " #1276 - Ben Shapiro                 0.00000      0.25213      0.00000   \n",
       "\n",
       "                                 component_4  component_5  \n",
       " #1383 - Malcolm Gladwell            0.00000      0.37409  \n",
       " #1382 - RZA & Donnell Rawlings      0.00077      0.00000  \n",
       " #1378 - Greg Fitzsimmons            0.00000      0.00000  \n",
       " #1375 - Edward Norton               0.34823      0.00000  \n",
       " #1373 - Kyle Kulinski               0.00000      0.01389  \n",
       "...                                      ...          ...  \n",
       " #1280 - Michael Yo                  0.00000      0.00000  \n",
       " #1279 - Jessimae Peluso             0.00000      0.00000  \n",
       " #1278 - Kevin Hart                  0.00000      0.00000  \n",
       " #1277 - Gabrielle Reece             0.00000      0.00000  \n",
       " #1276 - Ben Shapiro                 0.00000      0.00000  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = pd.DataFrame(doc_topic.round(5),\n",
    "             index = ps_ep_names,\n",
    "             columns = col_names)\n",
    "H"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
