{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python Version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# auto reload imports that change\n",
    "%load_ext autoreload\n",
    "# only set to auto reload for marked imports\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "%aimport credentials.cred\n",
    "from credentials import cred\n",
    "\n",
    "\n",
    "config = {\n",
    "  'host': cred.mongo_host,\n",
    "  'username': cred.mongo_user,\n",
    "  'password': cred.mongo_pass,\n",
    "  'authSource': cred.mongo_auth_db\n",
    "}\n",
    "\n",
    "# get a mongo client\n",
    "client = MongoClient(**config)\n",
    "\n",
    "# use the clean database\n",
    "jre_clean = client.jre_clean\n",
    "podcasts_clean = jre_clean.podcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Podcasts from Podscribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "ps_ep_mongo = list(\n",
    "    podcasts_clean\n",
    "    .find({},{'_id':0, 'number':1, 'name':1, 'desc':1, 'text':1, 'length':1, 'date':1, 'source':1, 'url':1})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# remove duplicates\n",
    "mongo_df = pd.DataFrame(ps_ep_mongo).sort_values(['name','source'], ascending=False).drop_duplicates(['name','number'])\n",
    "ps_ep_mongo = deepcopy(mongo_df.to_dict('records'))\n",
    "del mongo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out text and names\n",
    "ps_ep_text_list = [ep['text'] for ep in ps_ep_mongo]\n",
    "ps_ep_list = [' '.join([x['text'] for x in ep]) for ep in ps_ep_text_list]\n",
    "ps_ep_names = [f\"#{x['number']} \" + x['name'] for x in ps_ep_mongo]\n",
    "ps_ep_descs = [x['desc'] for x in ps_ep_mongo]\n",
    "ps_ep_dates = [x['date'] for x in ps_ep_mongo]\n",
    "ps_ep_lengths = [x['length'] for x in ps_ep_mongo]\n",
    "ps_ep_sources = [x['source'] for x in ps_ep_mongo]\n",
    "ps_ep_urls = [x['url'] for x in ps_ep_mongo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup clean\n",
    "clean_lemm = False\n",
    "spacy_ = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean_lemm:\n",
    "    ps_ep_list = [' '.join(simple_preprocess(x)) for x in ps_ep_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "new_stop_words = []\n",
    "\n",
    "# profanity\n",
    "profanity = (\n",
    "    open('stop_words/profanity.txt', newline='\\n')\n",
    "    .read()\n",
    "    .splitlines()\n",
    ")\n",
    "\n",
    "new_stop_words += [x.lower() for x in profanity]\n",
    "\n",
    "# common Joe Rogan words\n",
    "common_jre_words = [\n",
    "    'like', 'yeah', 'know', 'just', 'right', 'right', 'think', 'know',\n",
    "    'people', 'going', 'really', 'got', 'thing', 'want', 'actually',\n",
    "    'say', 'squarespace', 'sober', 'legalzoom', 'stamps', 'stamps.com',\n",
    "    'hmm', 'mmm', 'ha', 'com', 'whoop', 'october', 'um', 'uh', 'cash','app'\n",
    "]\n",
    "\n",
    "new_stop_words += common_jre_words\n",
    "\n",
    "# append to english stop words\n",
    "jre_stop_words = text.ENGLISH_STOP_WORDS.union(new_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "if clean_lemm:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for i, ep in enumerate(ps_ep_list):\n",
    "        ps_ep_list[i] = ' '.join([lemmatizer.lemmatize(x) for x in ep.split(' ')])\n",
    "\n",
    "    jre_stop_words = [lemmatizer.lemmatize(x) for x in jre_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spacy_:\n",
    "    import spacy\n",
    "    from spacy import displacy\n",
    "    from collections import Counter\n",
    "    import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "if spacy_:\n",
    "    ps_spacy = en_core_web_sm.load()\n",
    "\n",
    "    ps_spacy_list = []\n",
    "    tot = len(ps_ep_list)\n",
    "\n",
    "    for i, doc in enumerate(ps_ep_list):\n",
    "        clear_output()\n",
    "        print((i/tot)*100)\n",
    "        ps_spacy_list.append(ps_spacy(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spacy_:\n",
    "    for ent in ps_spacy_list[0].ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spacy_:\n",
    "    for ent in ps_spacy_list[0].ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spacy_:\n",
    "    selected_tokens = [\n",
    "        y for y in ps_spacy_list[0] if\n",
    "        (\n",
    "            not y.is_stop and\n",
    "            not y.is_punct and\n",
    "            not y.is_space\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tokenization\n",
    "tf_idf = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "if tf_idf:\n",
    "    # Create TF-IDF vectorizer\n",
    "    ps_ep_list_tfidf = TfidfVectorizer(\n",
    "        ngram_range=(1,3), binary=True, stop_words=jre_stop_words,\n",
    "        token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", max_df=0.35, min_df=0.01\n",
    "    )        \n",
    "\n",
    "    # fit\n",
    "    _ = ps_ep_list_tfidf.fit(ps_ep_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if not tf_idf:\n",
    "    # Create a CountVectorizer for parsing/counting words\n",
    "    ps_ep_list_cvec = CountVectorizer(\n",
    "        ngram_range=(1, 3), stop_words=jre_stop_words,\n",
    "        token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",  max_df=0.3, min_df=0.01\n",
    "    )\n",
    "\n",
    "    # fit\n",
    "    cvec_doc_word = ps_ep_list_cvec.fit_transform(ps_ep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tf_idf:\n",
    "    word = 'bc'\n",
    "    cvec_df = pd.DataFrame(cvec_doc_word.toarray(), index=ps_ep_names, columns=ps_ep_list_cvec.get_feature_names())\n",
    "#     display(\n",
    "#         cvec_df[cvec_df[word] >=1].loc[:,word].sort_values(ascending=False)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "if tf_idf:\n",
    "    ps_ep_list_tokenizer = ps_ep_list_tfidf\n",
    "else:\n",
    "    ps_ep_list_tokenizer = ps_ep_list_cvec\n",
    "\n",
    "# number of topics\n",
    "num_topics = 13\n",
    "\n",
    "# column names\n",
    "col_names = ['component_'+str(x) for x in range(1,num_topics+1)]\n",
    "\n",
    "# run LDA?\n",
    "run_lda = False\n",
    "run_lsa = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "if run_lda:\n",
    "    doc_word = ps_ep_list_tokenizer.transform(ps_ep_list).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "if run_lda:\n",
    "    corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lda:    \n",
    "    id2word = dict((v, k) for k, v in ps_ep_list_tokenizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "if run_lda:    \n",
    "    lda = models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lda:\n",
    "    lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lsa:\n",
    "    doc_word = ps_ep_list_tokenizer.transform(ps_ep_list)\n",
    "    doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "#  Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "if run_lsa:\n",
    "    lsa = TruncatedSVD(num_topics)\n",
    "    doc_topic = lsa.fit_transform(doc_word)\n",
    "    lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lsa:\n",
    "    topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "                 index = col_names,\n",
    "                 columns = ps_ep_list_tokenizer.get_feature_names())\n",
    "    topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nComponent \", ix+1)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i] if topic[i] >=0 else 'NOT '+feature_names[i]\n",
    "                        for i in abs(topic).argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lsa:\n",
    "    display_topics(lsa, ps_ep_list_tokenizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lsa:\n",
    "    Vt = pd.DataFrame(doc_topic.round(5),\n",
    "                 index = ps_ep_names,\n",
    "                 columns = col_names)\n",
    "    Vt.sort_values('component_2', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word = ps_ep_list_tokenizer.transform(ps_ep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(num_topics, random_state=42)\n",
    "doc_topic = nmf_model.fit_transform(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abby</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom share device</th>\n",
       "      <th>zoom video</th>\n",
       "      <th>zoom video communications</th>\n",
       "      <th>zoom video conferencing</th>\n",
       "      <th>zoom zoom</th>\n",
       "      <th>zoom zoom delivers</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zoos</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_7</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_9</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_10</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_11</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_12</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_13</th>\n",
       "      <td>0.046</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 57881 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 aa    aaa  aaron     ab  abandon  abandoned  abandoning  \\\n",
       "component_1   0.043  0.042  0.131  0.036    0.098      0.162       0.021   \n",
       "component_2   0.000  0.000  0.000  0.000    0.051      0.000       0.000   \n",
       "component_3   0.000  0.000  0.027  0.016    0.013      0.061       0.026   \n",
       "component_4   0.000  0.046  0.053  0.022    0.059      0.057       0.011   \n",
       "component_5   0.000  0.002  0.000  0.000    0.111      0.039       0.002   \n",
       "component_6   0.000  0.000  0.000  0.000    0.000      0.003       0.000   \n",
       "component_7   0.002  0.004  0.000  0.001    0.010      0.020       0.002   \n",
       "component_8   0.000  0.000  0.000  0.000    0.000      0.000       0.000   \n",
       "component_9   0.009  0.000  0.031  0.007    0.000      0.031       0.000   \n",
       "component_10  0.002  0.011  0.008  0.000    0.000      0.000       0.003   \n",
       "component_11  0.001  0.007  0.008  0.001    0.102      0.045       0.005   \n",
       "component_12  0.000  0.000  0.026  0.073    0.000      0.026       0.009   \n",
       "component_13  0.046  0.000  0.005  0.014    0.024      0.026       0.000   \n",
       "\n",
       "              abbott  abbreviated   abby  ...  zoom share device  zoom video  \\\n",
       "component_1    0.012        0.021  0.022  ...              0.019       0.086   \n",
       "component_2    0.000        0.000  0.000  ...              0.000       0.000   \n",
       "component_3    0.034        0.000  0.024  ...              0.000       0.002   \n",
       "component_4    0.001        0.000  0.012  ...              0.001       0.001   \n",
       "component_5    0.004        0.015  0.086  ...              0.000       0.001   \n",
       "component_6    0.047        0.000  0.055  ...              0.005       0.026   \n",
       "component_7    0.020        0.001  0.000  ...              0.004       0.008   \n",
       "component_8    0.000        0.000  0.000  ...              0.000       0.000   \n",
       "component_9    0.000        0.000  0.000  ...              0.037       0.123   \n",
       "component_10   0.000        0.000  0.036  ...              0.000       0.000   \n",
       "component_11   0.005        0.000  0.000  ...              0.000       0.000   \n",
       "component_12   0.008        0.000  0.000  ...              0.001       0.010   \n",
       "component_13   0.009        0.000  0.069  ...              0.000       0.002   \n",
       "\n",
       "              zoom video communications  zoom video conferencing  zoom zoom  \\\n",
       "component_1                       0.057                    0.028      0.040   \n",
       "component_2                       0.000                    0.000      0.000   \n",
       "component_3                       0.004                    0.000      0.000   \n",
       "component_4                       0.000                    0.002      0.005   \n",
       "component_5                       0.001                    0.000      0.000   \n",
       "component_6                       0.008                    0.017      0.006   \n",
       "component_7                       0.003                    0.005      0.001   \n",
       "component_8                       0.000                    0.000      0.006   \n",
       "component_9                       0.049                    0.074      0.112   \n",
       "component_10                      0.000                    0.000      0.000   \n",
       "component_11                      0.000                    0.000      0.008   \n",
       "component_12                      0.000                    0.010      0.000   \n",
       "component_13                      0.000                    0.001      0.020   \n",
       "\n",
       "              zoom zoom delivers  zoomed  zooms   zoos  zuckerberg  \n",
       "component_1                0.014   0.003  0.028  0.013       0.016  \n",
       "component_2                0.000   0.051  0.004  0.000       0.000  \n",
       "component_3                0.002   0.000  0.098  0.120       0.073  \n",
       "component_4                0.002   0.002  0.000  0.000       0.000  \n",
       "component_5                0.001   0.001  0.003  0.019       0.188  \n",
       "component_6                0.001   0.000  0.005  0.000       0.000  \n",
       "component_7                0.005   0.000  0.000  0.000       0.000  \n",
       "component_8                0.000   0.001  0.000  0.000       0.000  \n",
       "component_9                0.026   0.000  0.005  0.046       0.000  \n",
       "component_10               0.000   0.000  0.000  0.000       0.039  \n",
       "component_11               0.000   0.000  0.001  0.035       0.044  \n",
       "component_12               0.002   0.003  0.003  0.008       0.000  \n",
       "component_13               0.001   0.052  0.018  0.013       0.012  \n",
       "\n",
       "[13 rows x 57881 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = col_names,\n",
    "             columns = ps_ep_list_tokenizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Component  1\n",
      "comics, goddamn, netflix, coke, code joe, joey, comedians, bus, cop, yoga\n",
      "\n",
      "Component  2\n",
      "saturated, saturated fat, cholesterol, increase, randomized, protein, observational, heart disease, dietary, epidemiology\n",
      "\n",
      "Component  3\n",
      "deer, hunt, wildlife, bears, elk, hunters, wolves, cwd, hunter, bow\n",
      "\n",
      "Component  4\n",
      "ancient, civilization, climate, thousand years, sphinx, egypt, modern, ice age, flood, pyramid\n",
      "\n",
      "Component  5\n",
      "platform, speech, gender, trans, violence, policy, racist, wing, accounts, content\n",
      "\n",
      "Component  6\n",
      "calories, obesity, insulin, carbohydrate, carb, ketogenic, calorie, ketogenic diet, metabolic, body fat\n",
      "\n",
      "Component  7\n",
      "cannabis, marijuana, alcohol, heroin, medicine, cbd, addiction, schizophrenia, cocaine, thc\n",
      "\n",
      "Component  8\n",
      "quantum, mechanics, quantum mechanics, wave, function, physics, electron, worlds, particles, probability\n",
      "\n",
      "Component  9\n",
      "cells, diseases, stem, aging, plants, bacteria, vaccine, immune, vitamin, shown\n",
      "\n",
      "Component  10\n",
      "puerto, tax, rico, economy, puerto rico, taxes, income, vote, wage, rates\n",
      "\n",
      "Component  11\n",
      "simulation, intelligence, artificial, ai, artificial intelligence, systems, robot, biological, possibility, robots\n",
      "\n",
      "Component  12\n",
      "jitsu, jiu, jiu jitsu, wrestling, combat, technique, techniques, knee, skills, tim\n",
      "\n",
      "Component  13\n",
      "conspiracy, moon, aliens, cia, flat, alex, nazis, bullet, jones, vitamin\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, ps_ep_list_tokenizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_11</th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "      <th>component_3</th>\n",
       "      <th>component_4</th>\n",
       "      <th>component_5</th>\n",
       "      <th>component_6</th>\n",
       "      <th>component_7</th>\n",
       "      <th>component_8</th>\n",
       "      <th>component_9</th>\n",
       "      <th>component_10</th>\n",
       "      <th>component_12</th>\n",
       "      <th>component_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#1350 Nick Bostrom</th>\n",
       "      <td>10.67823</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1188 Lex Fridman</th>\n",
       "      <td>6.55706</td>\n",
       "      <td>0.07844</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.34034</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1211 Dr. Ben Goertzel</th>\n",
       "      <td>5.65799</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.28057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.24052</td>\n",
       "      <td>0.03552</td>\n",
       "      <td>0.14995</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1292 Lex Fridman</th>\n",
       "      <td>3.83396</td>\n",
       "      <td>0.47927</td>\n",
       "      <td>0.00155</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.32416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.55908</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1294 Jamie Metzl</th>\n",
       "      <td>3.48451</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11731</td>\n",
       "      <td>0.22861</td>\n",
       "      <td>0.45389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03576</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.77953</td>\n",
       "      <td>0.21593</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.24345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        component_11  component_1  component_2  component_3  \\\n",
       "#1350 Nick Bostrom          10.67823      0.00000      0.00000      0.00000   \n",
       "#1188 Lex Fridman            6.55706      0.07844      0.00000      0.00000   \n",
       "#1211 Dr. Ben Goertzel       5.65799      0.00000      0.00000      0.00000   \n",
       "#1292 Lex Fridman            3.83396      0.47927      0.00155      0.00000   \n",
       "#1294 Jamie Metzl            3.48451      0.00000      0.00000      0.11731   \n",
       "\n",
       "                        component_4  component_5  component_6  component_7  \\\n",
       "#1350 Nick Bostrom          0.00000      0.00000          0.0      0.00000   \n",
       "#1188 Lex Fridman           0.00000      0.00000          0.0      0.00000   \n",
       "#1211 Dr. Ben Goertzel      0.00000      0.28057          0.0      0.00000   \n",
       "#1292 Lex Fridman           0.00000      0.32416          0.0      0.00000   \n",
       "#1294 Jamie Metzl           0.22861      0.45389          0.0      0.03576   \n",
       "\n",
       "                        component_8  component_9  component_10  component_12  \\\n",
       "#1350 Nick Bostrom          0.00000      0.00000       0.00000       0.00000   \n",
       "#1188 Lex Fridman           0.00000      0.00000       0.00000       1.34034   \n",
       "#1211 Dr. Ben Goertzel      0.24052      0.03552       0.14995       0.00000   \n",
       "#1292 Lex Fridman           0.00000      0.00000       0.00000       1.55908   \n",
       "#1294 Jamie Metzl           0.00000      1.77953       0.21593       0.00000   \n",
       "\n",
       "                        component_13  \n",
       "#1350 Nick Bostrom           0.00000  \n",
       "#1188 Lex Fridman            0.00000  \n",
       "#1211 Dr. Ben Goertzel       0.00000  \n",
       "#1292 Lex Fridman            0.00000  \n",
       "#1294 Jamie Metzl            0.24345  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# select component\n",
    "nmf_comp = 'component_11'\n",
    "\n",
    "# make dataframe\n",
    "H = pd.DataFrame(doc_topic.round(5),\n",
    "             index = ps_ep_names,\n",
    "             columns = col_names)\n",
    "\n",
    "# get and re-order columns so selected component is first\n",
    "col_list = deepcopy(H.columns)\n",
    "new_index = pd.Index([nmf_comp]).append(col_list.drop(nmf_comp))\n",
    "\n",
    "# display with selected component sorted on and first\n",
    "H.sort_values(nmf_comp, ascending=False)[:5].loc[:,new_index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'component_1':'Writers/Entertainers', 'component_2':'Diet Science', 'component_3':'Hunting',\n",
    "    'component_4':'Ancient History', 'component_5':'Politics', 'component_6':'Diet Fitness', 'component_7':'Marijuana/Drugs',\n",
    "    'component_8':'Physics/Math', 'component_9':'Biology', 'component_10':'Economics',\n",
    "    'component_11':'AI/Tech', 'component_12':'Fighting', 'component_13':'CIA/Aliens/Conspiracy'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = H.rename(rename_dict, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df['Date'] = ps_ep_dates\n",
    "rec_df['Length'] = ps_ep_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_to_sec(time):\n",
    "    time_ls = time.split(':')\n",
    "    return int(time_ls[0])*3600 + int(time_ls[1])*60 + int(time_ls[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df['Length'] = rec_df['Length'].apply(len_to_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = rec_df.drop(['Length','Date'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_scaled = StandardScaler().fit_transform(rec_df.to_numpy())\n",
    "cosine_sim = linear_kernel(count_scaled, count_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_descs = pd.DataFrame([ps_ep_names,ps_ep_descs,ps_ep_urls]).transpose().set_index(0)\n",
    "del url_descs.index.name\n",
    "url_descs = url_descs.rename({1:'desc',2:'url'}, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas.io.sql as pd_sql\n",
    "\n",
    "# Postgres info to connect\n",
    "connection_args = {\n",
    "    'host': cred.sql_host,  # We are connecting to our _local_ version of psql\n",
    "    'port': cred.sql_port,         # port we opened on AWS\n",
    "    'dbname': 'jre_recs',\n",
    "    'user': cred.sql_user,\n",
    "    'password': cred.sql_pass\n",
    "}\n",
    "\n",
    "# sqlalchemy engine\n",
    "sql_eng = create_engine(\n",
    "    'postgresql+psycopg2://',\n",
    "    connect_args=connection_args\n",
    ")\n",
    "\n",
    "# connection\n",
    "sql_conn = sql_eng.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim.csv                                100%   15MB   1.4MB/s   00:10    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "894"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataframe\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim)\n",
    "\n",
    "# write to csv for postgres\n",
    "cosine_sim_df.to_csv('cosine_sim.csv', index=False, header=False)\n",
    "\n",
    "# copy to server\n",
    "!scp cosine_sim.csv ubuntu@myaws:./projects/project4/\n",
    "\n",
    "# make table with correct types\n",
    "pd_sql.to_sql(cosine_sim_df[:0],'cosine_sim',sql_conn.engine,if_exists='replace',index=False)\n",
    "\n",
    "# copy csv to table\n",
    "query = f'''\n",
    "COPY \"cosine_sim\" FROM '/home/ubuntu/projects/project4/cosine_sim.csv' DELIMITER ',' CSV;\n",
    "'''\n",
    "\n",
    "result = sql_conn.execute(query)\n",
    "sql_conn.execute('commit;')\n",
    "\n",
    "# show rows copied\n",
    "result.rowcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_df.csv                                    100%   94KB 972.8KB/s   00:00    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "894"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write to csv for postgres\n",
    "rec_df.to_csv('rec_df.csv', index=True, header=True)\n",
    "\n",
    "# copy to server\n",
    "!scp rec_df.csv ubuntu@myaws:./projects/project4/\n",
    "\n",
    "# make table with correct types\n",
    "pd_sql.to_sql(rec_df[:0],'rec_df',sql_conn.engine,if_exists='replace')\n",
    "\n",
    "# copy csv to table\n",
    "query = f'''\n",
    "COPY \"rec_df\" FROM '/home/ubuntu/projects/project4/rec_df.csv' DELIMITER ',' CSV HEADER;\n",
    "'''\n",
    "\n",
    "result = sql_conn.execute(query)\n",
    "sql_conn.execute('commit;')\n",
    "\n",
    "# show rows copied\n",
    "result.rowcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url_descs.csv                                 100%  248KB   1.4MB/s   00:00    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "894"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write to csv for postgres\n",
    "url_descs = url_descs.reset_index()\n",
    "url_descs.to_csv('url_descs.csv', index=True, header=True)\n",
    "\n",
    "# copy to server\n",
    "!scp url_descs.csv ubuntu@myaws:./projects/project4/\n",
    "\n",
    "# make table with correct types\n",
    "pd_sql.to_sql(url_descs[:0],'url_descs',sql_conn.engine,if_exists='replace')\n",
    "\n",
    "# copy csv to table\n",
    "query = f'''\n",
    "COPY \"url_descs\" FROM '/home/ubuntu/projects/project4/url_descs.csv' DELIMITER ',' CSV HEADER;\n",
    "'''\n",
    "\n",
    "result = sql_conn.execute(query)\n",
    "sql_conn.execute('commit;')\n",
    "\n",
    "# delete the indexing\n",
    "url_descs = url_descs.set_index('index')\n",
    "del url_descs.index.name\n",
    "\n",
    "# show rows copied\n",
    "result.rowcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class to Pull From Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recommender():\n",
    "    def __init__(self, sql_conn):\n",
    "        # cosine_sim query\n",
    "        query = '''\n",
    "        select *\n",
    "        from cosine_sim\n",
    "        '''\n",
    "        \n",
    "        # pull from sql and convert columns\n",
    "        self.cosine_sim = pd_sql.read_sql(query, sql_conn)\n",
    "        self.cosine_sim.columns = self.cosine_sim.columns.astype(int)\n",
    "        \n",
    "        # rec_df (topic_matrix) query\n",
    "        query = '''\n",
    "        select *\n",
    "        from rec_df\n",
    "        '''\n",
    "        \n",
    "        # get table, set index to podcast title column,\n",
    "        # delete the name of the index\n",
    "        self.topic_matrix = pd_sql.read_sql(query, sql_conn)\n",
    "        self.topic_matrix = self.topic_matrix.set_index('index')\n",
    "        del self.topic_matrix.index.name\n",
    "        \n",
    "        # url_descs query\n",
    "        query = '''\n",
    "        select *\n",
    "        from url_descs\n",
    "        '''\n",
    "        \n",
    "        # original iloc index pushed up to 'level_0',\n",
    "        # sort on and delete level_0\n",
    "        self.url_descs = pd_sql.read_sql(query, sql_conn).sort_values('level_0')\n",
    "        self.url_descs = self.url_descs.set_index('level_0').fillna('').set_index('index')\n",
    "        del self.url_descs.index.name\n",
    "    \n",
    "    def recommend_ep(self, title):\n",
    "        title_idx = self.topic_matrix.index.get_loc(title)\n",
    "        top_5_idx = list(\n",
    "            pd.Series(\n",
    "                self.cosine_sim[title_idx])\n",
    "            .sort_values(ascending = False)\n",
    "            .drop(title_idx)\n",
    "            [:5]\n",
    "            .index\n",
    "        )\n",
    "        \n",
    "        recs = list(\n",
    "            self.topic_matrix.iloc[top_5_idx,:]\n",
    "            .index\n",
    "        )\n",
    "        \n",
    "        tot_recs = self.url_descs.loc[recs,:].rename({1:'desc',2:'url'}, axis='columns')\n",
    "        \n",
    "        episodes = []\n",
    "        descs = []\n",
    "        urls = []\n",
    "        for k,v in tot_recs.to_dict('index').items():\n",
    "            episodes.append(k)\n",
    "            descs.append(v['desc'])\n",
    "            urls.append(v['url'])\n",
    "        res_dict = {'episodes':episodes, 'descs':descs, 'urls':urls}\n",
    "        return deepcopy(res_dict)\n",
    "    \n",
    "    def recommend_topic(self, topic):\n",
    "        recs = list(\n",
    "            self.topic_matrix[\n",
    "                self.topic_matrix.idxmax(axis='columns') == topic\n",
    "            ].sort_values(topic, ascending=False)\n",
    "            [:5]\n",
    "            .index\n",
    "        )\n",
    "        \n",
    "        tot_recs = self.url_descs.loc[recs,:].rename({1:'desc',2:'url'}, axis='columns')\n",
    "        episodes = []\n",
    "        descs = []\n",
    "        urls = []\n",
    "        for k,v in tot_recs.to_dict('index').items():\n",
    "            episodes.append(k)\n",
    "            descs.append(v['desc'])\n",
    "            urls.append(v['url'])\n",
    "        res_dict = {'episodes':episodes, 'descs':descs, 'urls':urls}\n",
    "        return deepcopy(res_dict)\n",
    "        \n",
    "    def get_topics(self):\n",
    "        return list(self.topic_matrix.columns.values)\n",
    "    \n",
    "    def get_ep_names(bself):\n",
    "        return list(self.topic_matrix.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2 as pg\n",
    "\n",
    "# get heroku database URL\n",
    "DATABASE_URL = cred.heroku_DB\n",
    "\n",
    "# Postgres connection\n",
    "sql_conn = pg.connect(DATABASE_URL, sslmode='require')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select *\n",
    "from rec_df\n",
    "'''\n",
    "\n",
    "# get table, set index to podcast title column,\n",
    "# delete the name of the index\n",
    "topic_matrix = pd_sql.read_sql(query, sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Writers/Entertainers</th>\n",
       "      <th>Diet Science</th>\n",
       "      <th>Hunting</th>\n",
       "      <th>Ancient History</th>\n",
       "      <th>Politics</th>\n",
       "      <th>Diet Fitness</th>\n",
       "      <th>Marijuana/Drugs</th>\n",
       "      <th>Physics/Math</th>\n",
       "      <th>Biology</th>\n",
       "      <th>Economics</th>\n",
       "      <th>AI/Tech</th>\n",
       "      <th>Fighting</th>\n",
       "      <th>CIA/Aliens/Conspiracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#1346 Zuby</td>\n",
       "      <td>0.68273</td>\n",
       "      <td>0.01143</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04338</td>\n",
       "      <td>1.03014</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01847</td>\n",
       "      <td>0.00934</td>\n",
       "      <td>0.09432</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04433</td>\n",
       "      <td>0.19127</td>\n",
       "      <td>0.39196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#1110 Zach Bitter</td>\n",
       "      <td>0.25416</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.07338</td>\n",
       "      <td>0.08212</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.81248</td>\n",
       "      <td>0.23030</td>\n",
       "      <td>0.09431</td>\n",
       "      <td>0.14670</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.83406</td>\n",
       "      <td>0.10396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#1392 Zach Bitter</td>\n",
       "      <td>0.12939</td>\n",
       "      <td>0.03753</td>\n",
       "      <td>0.09500</td>\n",
       "      <td>0.09279</td>\n",
       "      <td>0.01020</td>\n",
       "      <td>0.05101</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03457</td>\n",
       "      <td>0.12324</td>\n",
       "      <td>0.00662</td>\n",
       "      <td>0.01786</td>\n",
       "      <td>0.15314</td>\n",
       "      <td>0.00347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#984 Yvette d'Entremont</td>\n",
       "      <td>0.10438</td>\n",
       "      <td>0.04191</td>\n",
       "      <td>0.00130</td>\n",
       "      <td>0.06057</td>\n",
       "      <td>0.00570</td>\n",
       "      <td>0.07935</td>\n",
       "      <td>0.13793</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.27252</td>\n",
       "      <td>0.01056</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16933</td>\n",
       "      <td>0.09095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#69 Yves Edwards</td>\n",
       "      <td>0.14552</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04105</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.52835</td>\n",
       "      <td>0.00110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>#1111 Abby Martin</td>\n",
       "      <td>0.32615</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00459</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.86783</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.31613</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.26133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>#950 Abby Martin</td>\n",
       "      <td>0.06454</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00881</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.21382</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06555</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05473</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.45757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>#689 Abby Martin</td>\n",
       "      <td>0.07499</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.07281</td>\n",
       "      <td>0.12705</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04046</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01778</td>\n",
       "      <td>0.08461</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06175</td>\n",
       "      <td>0.33254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>#381 Abby Martin</td>\n",
       "      <td>0.10934</td>\n",
       "      <td>0.00048</td>\n",
       "      <td>0.05547</td>\n",
       "      <td>0.03860</td>\n",
       "      <td>0.06236</td>\n",
       "      <td>0.00725</td>\n",
       "      <td>0.03205</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.07376</td>\n",
       "      <td>0.08698</td>\n",
       "      <td>0.07680</td>\n",
       "      <td>0.00827</td>\n",
       "      <td>0.36389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>#643 \"Big\" Jay Oakerson</td>\n",
       "      <td>0.25477</td>\n",
       "      <td>0.00203</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01712</td>\n",
       "      <td>0.00739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00689</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00344</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02838</td>\n",
       "      <td>0.18908</td>\n",
       "      <td>0.03818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>894 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       index  Writers/Entertainers  Diet Science  Hunting  \\\n",
       "0                 #1346 Zuby               0.68273       0.01143  0.00000   \n",
       "1          #1110 Zach Bitter               0.25416       0.07348  0.07338   \n",
       "2          #1392 Zach Bitter               0.12939       0.03753  0.09500   \n",
       "3    #984 Yvette d'Entremont               0.10438       0.04191  0.00130   \n",
       "4           #69 Yves Edwards               0.14552       0.00000  0.04105   \n",
       "..                       ...                   ...           ...      ...   \n",
       "889        #1111 Abby Martin               0.32615       0.00000  0.00459   \n",
       "890         #950 Abby Martin               0.06454       0.00000  0.00881   \n",
       "891         #689 Abby Martin               0.07499       0.00000  0.00000   \n",
       "892         #381 Abby Martin               0.10934       0.00048  0.05547   \n",
       "893  #643 \"Big\" Jay Oakerson               0.25477       0.00203  0.00000   \n",
       "\n",
       "     Ancient History  Politics  Diet Fitness  Marijuana/Drugs  Physics/Math  \\\n",
       "0            0.04338   1.03014       0.00000          0.01847       0.00934   \n",
       "1            0.08212   0.00000       0.81248          0.23030       0.09431   \n",
       "2            0.09279   0.01020       0.05101          0.00000       0.03457   \n",
       "3            0.06057   0.00570       0.07935          0.13793       0.00000   \n",
       "4            0.00000   0.00000       0.00003          0.00000       0.00000   \n",
       "..               ...       ...           ...              ...           ...   \n",
       "889          0.00000   0.86783       0.00000          0.00000       0.00000   \n",
       "890          0.00000   0.21382       0.00000          0.06555       0.00000   \n",
       "891          0.07281   0.12705       0.00000          0.04046       0.00000   \n",
       "892          0.03860   0.06236       0.00725          0.03205       0.00000   \n",
       "893          0.01712   0.00739       0.00000          0.00689       0.00000   \n",
       "\n",
       "     Biology  Economics  AI/Tech  Fighting  CIA/Aliens/Conspiracy  \n",
       "0    0.09432    0.00000  0.04433   0.19127                0.39196  \n",
       "1    0.14670    0.00000  0.00000   0.83406                0.10396  \n",
       "2    0.12324    0.00662  0.01786   0.15314                0.00347  \n",
       "3    0.27252    0.01056  0.00000   0.16933                0.09095  \n",
       "4    0.00000    0.00000  0.00000   0.52835                0.00110  \n",
       "..       ...        ...      ...       ...                    ...  \n",
       "889  0.00000    0.31613  0.00000   0.00000                1.26133  \n",
       "890  0.00000    0.05473  0.00000   0.00000                0.45757  \n",
       "891  0.01778    0.08461  0.00000   0.06175                0.33254  \n",
       "892  0.07376    0.08698  0.07680   0.00827                0.36389  \n",
       "893  0.00344    0.00000  0.02838   0.18908                0.03818  \n",
       "\n",
       "[894 rows x 14 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_matrix = topic_matrix.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['#1346 Zuby', '#1110 Zach Bitter', '#1392 Zach Bitter',\n",
       "       '#984 Yvette d'Entremont', '#69 Yves Edwards', '#20 Yves Edwards',\n",
       "       '#17 Yoel Romero & Joey Diaz', '#1306 Wiz Khalifa',\n",
       "       '#1201 William von Hippel', '#930 Will MacAskill',\n",
       "       ...\n",
       "       '#1028 Adam Greentree', '#1130 Adam Frank', '#1282 Adam Conover',\n",
       "       '#475 Adam Carolla', '#1316 Abby Martin', '#1111 Abby Martin',\n",
       "       '#950 Abby Martin', '#689 Abby Martin', '#381 Abby Martin',\n",
       "       '#643 \"Big\" Jay Oakerson'],\n",
       "      dtype='object', name='index', length=894)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "del topic_matrix.index.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['#1346 Zuby', '#1110 Zach Bitter', '#1392 Zach Bitter',\n",
       "       '#984 Yvette d'Entremont', '#69 Yves Edwards', '#20 Yves Edwards',\n",
       "       '#17 Yoel Romero & Joey Diaz', '#1306 Wiz Khalifa',\n",
       "       '#1201 William von Hippel', '#930 Will MacAskill',\n",
       "       ...\n",
       "       '#1028 Adam Greentree', '#1130 Adam Frank', '#1282 Adam Conover',\n",
       "       '#475 Adam Carolla', '#1316 Abby Martin', '#1111 Abby Martin',\n",
       "       '#950 Abby Martin', '#689 Abby Martin', '#381 Abby Martin',\n",
       "       '#643 \"Big\" Jay Oakerson'],\n",
       "      dtype='object', length=894)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create recommender\n",
    "my_recommender = recommender(sql_conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
