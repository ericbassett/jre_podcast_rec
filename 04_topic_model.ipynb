{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python Version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# auto reload imports that change\n",
    "%load_ext autoreload\n",
    "# only set to auto reload for marked imports\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "%aimport credentials.cred\n",
    "from credentials import cred\n",
    "\n",
    "\n",
    "config = {\n",
    "  'host': cred.mongo_host,\n",
    "  'username': cred.mongo_user,\n",
    "  'password': cred.mongo_pass,\n",
    "  'authSource': cred.mongo_auth_db\n",
    "}\n",
    "\n",
    "# get a mongo client\n",
    "client = MongoClient(**config)\n",
    "\n",
    "# use the clean database\n",
    "jre_clean = client.jre_clean\n",
    "podcasts_clean = jre_clean.podcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Podcasts from Podscribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "ps_ep_mongo = list(\n",
    "    podcasts_clean\n",
    "    .find({},{'_id':0, 'number':1, 'name':1, 'desc':1, 'text':1, 'length':1, 'date':1, 'source':1, 'url':1})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# remove duplicates\n",
    "mongo_df = pd.DataFrame(ps_ep_mongo).sort_values(['name','source'], ascending=False).drop_duplicates(['name','number'])\n",
    "ps_ep_mongo = deepcopy(mongo_df.to_dict('records'))\n",
    "del mongo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out text and names\n",
    "ps_ep_text_list = [ep['text'] for ep in ps_ep_mongo]\n",
    "ps_ep_list = [' '.join([x['text'] for x in ep]) for ep in ps_ep_text_list]\n",
    "ps_ep_names = [f\"#{x['number']} \" + x['name'] for x in ps_ep_mongo]\n",
    "ps_ep_descs = [x['desc'] for x in ps_ep_mongo]\n",
    "ps_ep_dates = [x['date'] for x in ps_ep_mongo]\n",
    "ps_ep_lengths = [x['length'] for x in ps_ep_mongo]\n",
    "ps_ep_sources = [x['source'] for x in ps_ep_mongo]\n",
    "ps_ep_urls = [x['url'] for x in ps_ep_mongo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup clean\n",
    "clean_lemm = False\n",
    "spacy_ = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean_lemm:\n",
    "    ps_ep_list = [' '.join(simple_preprocess(x)) for x in ps_ep_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "new_stop_words = []\n",
    "\n",
    "# profanity\n",
    "profanity = (\n",
    "    open('stop_words/profanity.txt', newline='\\n')\n",
    "    .read()\n",
    "    .splitlines()\n",
    ")\n",
    "\n",
    "new_stop_words += [x.lower() for x in profanity]\n",
    "\n",
    "# common Joe Rogan words\n",
    "common_jre_words = [\n",
    "    'like', 'yeah', 'know', 'just', 'right', 'right', 'think', 'know',\n",
    "    'people', 'going', 'really', 'got', 'thing', 'want', 'actually',\n",
    "    'say', 'squarespace', 'sober', 'legalzoom', 'stamps', 'stamps.com',\n",
    "    'hmm', 'mmm', 'ha', 'com', 'whoop', 'october', 'um', 'uh', 'cash','app'\n",
    "]\n",
    "\n",
    "new_stop_words += common_jre_words\n",
    "\n",
    "# append to english stop words\n",
    "jre_stop_words = text.ENGLISH_STOP_WORDS.union(new_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "if clean_lemm:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for i, ep in enumerate(ps_ep_list):\n",
    "        ps_ep_list[i] = ' '.join([lemmatizer.lemmatize(x) for x in ep.split(' ')])\n",
    "\n",
    "    jre_stop_words = [lemmatizer.lemmatize(x) for x in jre_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spacy_:\n",
    "    import spacy\n",
    "    from spacy import displacy\n",
    "    from collections import Counter\n",
    "    import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "if spacy_:\n",
    "    ps_spacy = en_core_web_sm.load()\n",
    "\n",
    "    ps_spacy_list = []\n",
    "    tot = len(ps_ep_list)\n",
    "\n",
    "    for i, doc in enumerate(ps_ep_list):\n",
    "        clear_output()\n",
    "        print((i/tot)*100)\n",
    "        ps_spacy_list.append(ps_spacy(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spacy_:\n",
    "    for ent in ps_spacy_list[0].ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spacy_:\n",
    "    for ent in ps_spacy_list[0].ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spacy_:\n",
    "    selected_tokens = [\n",
    "        y for y in ps_spacy_list[0] if\n",
    "        (\n",
    "            not y.is_stop and\n",
    "            not y.is_punct and\n",
    "            not y.is_space\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tokenization\n",
    "tf_idf = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "if tf_idf:\n",
    "    # Create TF-IDF vectorizer\n",
    "    ps_ep_list_tfidf = TfidfVectorizer(\n",
    "        ngram_range=(1,3), binary=True, stop_words=jre_stop_words,\n",
    "        token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", max_df=0.35, min_df=0.01\n",
    "    )        \n",
    "\n",
    "    # fit\n",
    "    _ = ps_ep_list_tfidf.fit(ps_ep_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if not tf_idf:\n",
    "    # Create a CountVectorizer for parsing/counting words\n",
    "    ps_ep_list_cvec = CountVectorizer(\n",
    "        ngram_range=(1, 3), stop_words=jre_stop_words,\n",
    "        token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",  max_df=0.3, min_df=0.01\n",
    "    )\n",
    "\n",
    "    # fit\n",
    "    cvec_doc_word = ps_ep_list_cvec.fit_transform(ps_ep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tf_idf:\n",
    "    word = 'bc'\n",
    "    cvec_df = pd.DataFrame(cvec_doc_word.toarray(), index=ps_ep_names, columns=ps_ep_list_cvec.get_feature_names())\n",
    "#     display(\n",
    "#         cvec_df[cvec_df[word] >=1].loc[:,word].sort_values(ascending=False)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "if tf_idf:\n",
    "    ps_ep_list_tokenizer = ps_ep_list_tfidf\n",
    "else:\n",
    "    ps_ep_list_tokenizer = ps_ep_list_cvec\n",
    "\n",
    "# number of topics\n",
    "num_topics = 13\n",
    "\n",
    "# column names\n",
    "col_names = ['component_'+str(x) for x in range(1,num_topics+1)]\n",
    "\n",
    "# run LDA?\n",
    "run_lda = False\n",
    "run_lsa = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "if run_lda:\n",
    "    doc_word = ps_ep_list_tokenizer.transform(ps_ep_list).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "if run_lda:\n",
    "    corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lda:    \n",
    "    id2word = dict((v, k) for k, v in ps_ep_list_tokenizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "if run_lda:    \n",
    "    lda = models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lda:\n",
    "    lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lsa:\n",
    "    doc_word = ps_ep_list_tokenizer.transform(ps_ep_list)\n",
    "    doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "#  Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "if run_lsa:\n",
    "    lsa = TruncatedSVD(num_topics)\n",
    "    doc_topic = lsa.fit_transform(doc_word)\n",
    "    lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lsa:\n",
    "    topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "                 index = col_names,\n",
    "                 columns = ps_ep_list_tokenizer.get_feature_names())\n",
    "    topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nComponent \", ix+1)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i] if topic[i] >=0 else 'NOT '+feature_names[i]\n",
    "                        for i in abs(topic).argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lsa:\n",
    "    display_topics(lsa, ps_ep_list_tokenizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_lsa:\n",
    "    Vt = pd.DataFrame(doc_topic.round(5),\n",
    "                 index = ps_ep_names,\n",
    "                 columns = col_names)\n",
    "    Vt.sort_values('component_2', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word = ps_ep_list_tokenizer.transform(ps_ep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(num_topics, random_state=42)\n",
    "doc_topic = nmf_model.fit_transform(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abby</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom share device</th>\n",
       "      <th>zoom video</th>\n",
       "      <th>zoom video communications</th>\n",
       "      <th>zoom video conferencing</th>\n",
       "      <th>zoom zoom</th>\n",
       "      <th>zoom zoom delivers</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zoos</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_7</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_9</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_10</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_11</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_12</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_13</th>\n",
       "      <td>0.046</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 57881 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 aa    aaa  aaron     ab  abandon  abandoned  abandoning  \\\n",
       "component_1   0.043  0.042  0.131  0.036    0.098      0.162       0.021   \n",
       "component_2   0.000  0.000  0.000  0.000    0.051      0.000       0.000   \n",
       "component_3   0.000  0.000  0.027  0.016    0.013      0.061       0.026   \n",
       "component_4   0.000  0.046  0.053  0.022    0.059      0.057       0.011   \n",
       "component_5   0.000  0.002  0.000  0.000    0.111      0.039       0.002   \n",
       "component_6   0.000  0.000  0.000  0.000    0.000      0.003       0.000   \n",
       "component_7   0.002  0.004  0.000  0.001    0.010      0.020       0.002   \n",
       "component_8   0.000  0.000  0.000  0.000    0.000      0.000       0.000   \n",
       "component_9   0.009  0.000  0.031  0.007    0.000      0.031       0.000   \n",
       "component_10  0.002  0.011  0.008  0.000    0.000      0.000       0.003   \n",
       "component_11  0.001  0.007  0.008  0.001    0.102      0.045       0.005   \n",
       "component_12  0.000  0.000  0.026  0.073    0.000      0.026       0.009   \n",
       "component_13  0.046  0.000  0.005  0.014    0.024      0.026       0.000   \n",
       "\n",
       "              abbott  abbreviated   abby  ...  zoom share device  zoom video  \\\n",
       "component_1    0.012        0.021  0.022  ...              0.019       0.086   \n",
       "component_2    0.000        0.000  0.000  ...              0.000       0.000   \n",
       "component_3    0.034        0.000  0.024  ...              0.000       0.002   \n",
       "component_4    0.001        0.000  0.012  ...              0.001       0.001   \n",
       "component_5    0.004        0.015  0.086  ...              0.000       0.001   \n",
       "component_6    0.047        0.000  0.055  ...              0.005       0.026   \n",
       "component_7    0.020        0.001  0.000  ...              0.004       0.008   \n",
       "component_8    0.000        0.000  0.000  ...              0.000       0.000   \n",
       "component_9    0.000        0.000  0.000  ...              0.037       0.123   \n",
       "component_10   0.000        0.000  0.036  ...              0.000       0.000   \n",
       "component_11   0.005        0.000  0.000  ...              0.000       0.000   \n",
       "component_12   0.008        0.000  0.000  ...              0.001       0.010   \n",
       "component_13   0.009        0.000  0.069  ...              0.000       0.002   \n",
       "\n",
       "              zoom video communications  zoom video conferencing  zoom zoom  \\\n",
       "component_1                       0.057                    0.028      0.040   \n",
       "component_2                       0.000                    0.000      0.000   \n",
       "component_3                       0.004                    0.000      0.000   \n",
       "component_4                       0.000                    0.002      0.005   \n",
       "component_5                       0.001                    0.000      0.000   \n",
       "component_6                       0.008                    0.017      0.006   \n",
       "component_7                       0.003                    0.005      0.001   \n",
       "component_8                       0.000                    0.000      0.006   \n",
       "component_9                       0.049                    0.074      0.112   \n",
       "component_10                      0.000                    0.000      0.000   \n",
       "component_11                      0.000                    0.000      0.008   \n",
       "component_12                      0.000                    0.010      0.000   \n",
       "component_13                      0.000                    0.001      0.020   \n",
       "\n",
       "              zoom zoom delivers  zoomed  zooms   zoos  zuckerberg  \n",
       "component_1                0.014   0.003  0.028  0.013       0.016  \n",
       "component_2                0.000   0.051  0.004  0.000       0.000  \n",
       "component_3                0.002   0.000  0.098  0.120       0.073  \n",
       "component_4                0.002   0.002  0.000  0.000       0.000  \n",
       "component_5                0.001   0.001  0.003  0.019       0.188  \n",
       "component_6                0.001   0.000  0.005  0.000       0.000  \n",
       "component_7                0.005   0.000  0.000  0.000       0.000  \n",
       "component_8                0.000   0.001  0.000  0.000       0.000  \n",
       "component_9                0.026   0.000  0.005  0.046       0.000  \n",
       "component_10               0.000   0.000  0.000  0.000       0.039  \n",
       "component_11               0.000   0.000  0.001  0.035       0.044  \n",
       "component_12               0.002   0.003  0.003  0.008       0.000  \n",
       "component_13               0.001   0.052  0.018  0.013       0.012  \n",
       "\n",
       "[13 rows x 57881 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = col_names,\n",
    "             columns = ps_ep_list_tokenizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Component  1\n",
      "comics, goddamn, netflix, coke, code joe, joey, comedians, bus, cop, yoga\n",
      "\n",
      "Component  2\n",
      "saturated, saturated fat, cholesterol, increase, randomized, protein, observational, heart disease, dietary, epidemiology\n",
      "\n",
      "Component  3\n",
      "deer, hunt, wildlife, bears, elk, hunters, wolves, cwd, hunter, bow\n",
      "\n",
      "Component  4\n",
      "ancient, civilization, climate, thousand years, sphinx, egypt, modern, ice age, flood, pyramid\n",
      "\n",
      "Component  5\n",
      "platform, speech, gender, trans, violence, policy, racist, wing, accounts, content\n",
      "\n",
      "Component  6\n",
      "calories, obesity, insulin, carbohydrate, carb, ketogenic, calorie, ketogenic diet, metabolic, body fat\n",
      "\n",
      "Component  7\n",
      "cannabis, marijuana, alcohol, heroin, medicine, cbd, addiction, schizophrenia, cocaine, thc\n",
      "\n",
      "Component  8\n",
      "quantum, mechanics, quantum mechanics, wave, function, physics, electron, worlds, particles, probability\n",
      "\n",
      "Component  9\n",
      "cells, diseases, stem, aging, plants, bacteria, vaccine, immune, vitamin, shown\n",
      "\n",
      "Component  10\n",
      "puerto, tax, rico, economy, puerto rico, taxes, income, vote, wage, rates\n",
      "\n",
      "Component  11\n",
      "simulation, intelligence, artificial, ai, artificial intelligence, systems, robot, biological, possibility, robots\n",
      "\n",
      "Component  12\n",
      "jitsu, jiu, jiu jitsu, wrestling, combat, technique, techniques, knee, skills, tim\n",
      "\n",
      "Component  13\n",
      "conspiracy, moon, aliens, cia, flat, alex, nazis, bullet, jones, vitamin\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, ps_ep_list_tokenizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_11</th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "      <th>component_3</th>\n",
       "      <th>component_4</th>\n",
       "      <th>component_5</th>\n",
       "      <th>component_6</th>\n",
       "      <th>component_7</th>\n",
       "      <th>component_8</th>\n",
       "      <th>component_9</th>\n",
       "      <th>component_10</th>\n",
       "      <th>component_12</th>\n",
       "      <th>component_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#1350 Nick Bostrom</th>\n",
       "      <td>10.67823</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1188 Lex Fridman</th>\n",
       "      <td>6.55706</td>\n",
       "      <td>0.07844</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.34034</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1211 Dr. Ben Goertzel</th>\n",
       "      <td>5.65799</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.28057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.24052</td>\n",
       "      <td>0.03552</td>\n",
       "      <td>0.14995</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1292 Lex Fridman</th>\n",
       "      <td>3.83396</td>\n",
       "      <td>0.47927</td>\n",
       "      <td>0.00155</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.32416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.55908</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1294 Jamie Metzl</th>\n",
       "      <td>3.48451</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11731</td>\n",
       "      <td>0.22861</td>\n",
       "      <td>0.45389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03576</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.77953</td>\n",
       "      <td>0.21593</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.24345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        component_11  component_1  component_2  component_3  \\\n",
       "#1350 Nick Bostrom          10.67823      0.00000      0.00000      0.00000   \n",
       "#1188 Lex Fridman            6.55706      0.07844      0.00000      0.00000   \n",
       "#1211 Dr. Ben Goertzel       5.65799      0.00000      0.00000      0.00000   \n",
       "#1292 Lex Fridman            3.83396      0.47927      0.00155      0.00000   \n",
       "#1294 Jamie Metzl            3.48451      0.00000      0.00000      0.11731   \n",
       "\n",
       "                        component_4  component_5  component_6  component_7  \\\n",
       "#1350 Nick Bostrom          0.00000      0.00000          0.0      0.00000   \n",
       "#1188 Lex Fridman           0.00000      0.00000          0.0      0.00000   \n",
       "#1211 Dr. Ben Goertzel      0.00000      0.28057          0.0      0.00000   \n",
       "#1292 Lex Fridman           0.00000      0.32416          0.0      0.00000   \n",
       "#1294 Jamie Metzl           0.22861      0.45389          0.0      0.03576   \n",
       "\n",
       "                        component_8  component_9  component_10  component_12  \\\n",
       "#1350 Nick Bostrom          0.00000      0.00000       0.00000       0.00000   \n",
       "#1188 Lex Fridman           0.00000      0.00000       0.00000       1.34034   \n",
       "#1211 Dr. Ben Goertzel      0.24052      0.03552       0.14995       0.00000   \n",
       "#1292 Lex Fridman           0.00000      0.00000       0.00000       1.55908   \n",
       "#1294 Jamie Metzl           0.00000      1.77953       0.21593       0.00000   \n",
       "\n",
       "                        component_13  \n",
       "#1350 Nick Bostrom           0.00000  \n",
       "#1188 Lex Fridman            0.00000  \n",
       "#1211 Dr. Ben Goertzel       0.00000  \n",
       "#1292 Lex Fridman            0.00000  \n",
       "#1294 Jamie Metzl            0.24345  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# select component\n",
    "nmf_comp = 'component_11'\n",
    "\n",
    "# make dataframe\n",
    "H = pd.DataFrame(doc_topic.round(5),\n",
    "             index = ps_ep_names,\n",
    "             columns = col_names)\n",
    "\n",
    "# get and re-order columns so selected component is first\n",
    "col_list = deepcopy(H.columns)\n",
    "new_index = pd.Index([nmf_comp]).append(col_list.drop(nmf_comp))\n",
    "\n",
    "# display with selected component sorted on and first\n",
    "H.sort_values(nmf_comp, ascending=False)[:5].loc[:,new_index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'component_1':'Writers/Entertainers', 'component_2':'Diet Science', 'component_3':'Hunting',\n",
    "    'component_4':'Ancient History', 'component_5':'Politics', 'component_6':'Diet Fitness', 'component_7':'Marijuana/Drugs',\n",
    "    'component_8':'Physics/Math', 'component_9':'Biology', 'component_10':'Economics',\n",
    "    'component_11':'AI/Tech', 'component_12':'Fighting', 'component_13':'CIA/Aliens/Conspiracy'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = H.rename(rename_dict, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df['Date'] = ps_ep_dates\n",
    "rec_df['Length'] = ps_ep_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_to_sec(time):\n",
    "    time_ls = time.split(':')\n",
    "    return int(time_ls[0])*3600 + int(time_ls[1])*60 + int(time_ls[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df['Length'] = rec_df['Length'].apply(len_to_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = rec_df.drop(['Length','Date'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_scaled = StandardScaler().fit_transform(rec_df.to_numpy())\n",
    "cosine_sim = linear_kernel(count_scaled, count_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recommender():\n",
    "    def __init__(self, cosine_sim, topic_matrix, url_descs):\n",
    "        self.cosine_sim = deepcopy(cosine_sim)\n",
    "        self.topic_matrix = deepcopy(topic_matrix)\n",
    "        self.url_descs = deepcopy(url_descs)\n",
    "        \n",
    "    def recommend_ep(self, title):\n",
    "        title_idx = self.topic_matrix.index.get_loc(title)\n",
    "        top_5_idx = list(\n",
    "            pd.Series(\n",
    "                self.cosine_sim[title_idx])\n",
    "            .sort_values(ascending = False)\n",
    "            .drop(title_idx)\n",
    "            [:5]\n",
    "            .index\n",
    "        )\n",
    "        \n",
    "        print(top_5_idx)\n",
    "        recs = list(\n",
    "            self.topic_matrix.iloc[top_5_idx,:]\n",
    "            .index\n",
    "        )\n",
    "        \n",
    "        tot_recs = self.url_descs.loc[recs,:].rename({1:'desc',2:'url'}, axis='columns')\n",
    "        \n",
    "        episodes = []\n",
    "        descs = []\n",
    "        urls = []\n",
    "        for k,v in tot_recs.to_dict('index').items():\n",
    "            episodes.append(k)\n",
    "            descs.append(v['desc'])\n",
    "            urls.append(v['url'])\n",
    "        res_dict = {'episodes':episodes, 'descs':descs, 'urls':urls}\n",
    "        return deepcopy(res_dict)\n",
    "    \n",
    "    def recommend_topic(self, topic):\n",
    "        recs = list(\n",
    "            self.topic_matrix[\n",
    "                self.topic_matrix.idxmax(axis='columns') == topic\n",
    "            ].sort_values(topic, ascending=False)\n",
    "            [:5]\n",
    "            .index\n",
    "        )\n",
    "        \n",
    "        tot_recs = self.url_descs.loc[recs,:].rename({1:'desc',2:'url'}, axis='columns')\n",
    "        episodes = []\n",
    "        descs = []\n",
    "        urls = []\n",
    "        for k,v in tot_recs.to_dict('index').items():\n",
    "            episodes.append(k)\n",
    "            descs.append(v['desc'])\n",
    "            urls.append(v['url'])\n",
    "        res_dict = {'episodes':episodes, 'descs':descs, 'urls':urls}\n",
    "        return deepcopy(res_dict)\n",
    "        \n",
    "    def get_topics(self):\n",
    "        return list(self.topic_matrix.columns.values)\n",
    "    \n",
    "    def get_ep_names(self):\n",
    "        return list(self.topic_matrix.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Writers/Entertainers</th>\n",
       "      <th>Diet Science</th>\n",
       "      <th>Hunting</th>\n",
       "      <th>Ancient History</th>\n",
       "      <th>Politics</th>\n",
       "      <th>Diet Fitness</th>\n",
       "      <th>Marijuana/Drugs</th>\n",
       "      <th>Physics/Math</th>\n",
       "      <th>Biology</th>\n",
       "      <th>Economics</th>\n",
       "      <th>AI/Tech</th>\n",
       "      <th>Fighting</th>\n",
       "      <th>CIA/Aliens/Conspiracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#1346 Zuby</th>\n",
       "      <td>0.68273</td>\n",
       "      <td>0.01143</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04338</td>\n",
       "      <td>1.03014</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01847</td>\n",
       "      <td>0.00934</td>\n",
       "      <td>0.09432</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04433</td>\n",
       "      <td>0.19127</td>\n",
       "      <td>0.39196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1110 Zach Bitter</th>\n",
       "      <td>0.25416</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.07338</td>\n",
       "      <td>0.08212</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.81248</td>\n",
       "      <td>0.23030</td>\n",
       "      <td>0.09431</td>\n",
       "      <td>0.14670</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.83406</td>\n",
       "      <td>0.10396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1392 Zach Bitter</th>\n",
       "      <td>0.12939</td>\n",
       "      <td>0.03753</td>\n",
       "      <td>0.09500</td>\n",
       "      <td>0.09279</td>\n",
       "      <td>0.01020</td>\n",
       "      <td>0.05101</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03457</td>\n",
       "      <td>0.12324</td>\n",
       "      <td>0.00662</td>\n",
       "      <td>0.01786</td>\n",
       "      <td>0.15314</td>\n",
       "      <td>0.00347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#984 Yvette d'Entremont</th>\n",
       "      <td>0.10438</td>\n",
       "      <td>0.04191</td>\n",
       "      <td>0.00130</td>\n",
       "      <td>0.06057</td>\n",
       "      <td>0.00570</td>\n",
       "      <td>0.07935</td>\n",
       "      <td>0.13793</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.27252</td>\n",
       "      <td>0.01056</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16933</td>\n",
       "      <td>0.09095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#69 Yves Edwards</th>\n",
       "      <td>0.14552</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04105</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.52835</td>\n",
       "      <td>0.00110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Writers/Entertainers  Diet Science  Hunting  \\\n",
       "#1346 Zuby                            0.68273       0.01143  0.00000   \n",
       "#1110 Zach Bitter                     0.25416       0.07348  0.07338   \n",
       "#1392 Zach Bitter                     0.12939       0.03753  0.09500   \n",
       "#984 Yvette d'Entremont               0.10438       0.04191  0.00130   \n",
       "#69 Yves Edwards                      0.14552       0.00000  0.04105   \n",
       "\n",
       "                         Ancient History  Politics  Diet Fitness  \\\n",
       "#1346 Zuby                       0.04338   1.03014       0.00000   \n",
       "#1110 Zach Bitter                0.08212   0.00000       0.81248   \n",
       "#1392 Zach Bitter                0.09279   0.01020       0.05101   \n",
       "#984 Yvette d'Entremont          0.06057   0.00570       0.07935   \n",
       "#69 Yves Edwards                 0.00000   0.00000       0.00003   \n",
       "\n",
       "                         Marijuana/Drugs  Physics/Math  Biology  Economics  \\\n",
       "#1346 Zuby                       0.01847       0.00934  0.09432    0.00000   \n",
       "#1110 Zach Bitter                0.23030       0.09431  0.14670    0.00000   \n",
       "#1392 Zach Bitter                0.00000       0.03457  0.12324    0.00662   \n",
       "#984 Yvette d'Entremont          0.13793       0.00000  0.27252    0.01056   \n",
       "#69 Yves Edwards                 0.00000       0.00000  0.00000    0.00000   \n",
       "\n",
       "                         AI/Tech  Fighting  CIA/Aliens/Conspiracy  \n",
       "#1346 Zuby               0.04433   0.19127                0.39196  \n",
       "#1110 Zach Bitter        0.00000   0.83406                0.10396  \n",
       "#1392 Zach Bitter        0.01786   0.15314                0.00347  \n",
       "#984 Yvette d'Entremont  0.00000   0.16933                0.09095  \n",
       "#69 Yves Edwards         0.00000   0.52835                0.00110  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_descs = pd.DataFrame([ps_ep_names,ps_ep_descs,ps_ep_urls]).transpose().set_index(0)\n",
    "del url_descs.index.name\n",
    "url_descs.rename({1:'desc',2:'url'}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "del url_descs.index.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = recommender(cosine_sim, rec_df, url_descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Writers/Entertainers',\n",
       " 'Diet Science',\n",
       " 'Hunting',\n",
       " 'Ancient History',\n",
       " 'Politics',\n",
       " 'Diet Fitness',\n",
       " 'Marijuana/Drugs',\n",
       " 'Physics/Math',\n",
       " 'Biology',\n",
       " 'Economics',\n",
       " 'AI/Tech',\n",
       " 'Fighting',\n",
       " 'CIA/Aliens/Conspiracy']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = a.recommend_topic('Writers/Entertainers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episodes': ['#1310 Sober October 2019 Preview',\n",
       "  '#1355 Mark Normand',\n",
       "  '#1293 Andrew Santino',\n",
       "  '#1354 The Black Keys',\n",
       "  '#1319 Joey Diaz'],\n",
       " 'descs': [\"Joe is joined by Ari Shaffir, Tom Segura & Bert Kreischer to discuss this year's Sober October challenge.\",\n",
       "  'Mark Normand is a stand-up comedian and actor. Check out his podcast \"Tuesdays with Stories!\" with co-host Joe List available on Apple Podcasts.',\n",
       "  'Andrew Santino\\xa0is a stand up comedian and actor. Check out his podcast \"Whiskey Ginger\" available on Apple Podcasts & YouTube.',\n",
       "  'The Black Keys is a rock band formed in Akron, Ohio, in 2001. The group consists of Dan Auerbach and Patrick Carney. Their latest album \"Let\\'s Rock\" is available now everywhere.',\n",
       "  'Joey “CoCo” Diaz is a Cuban-American stand up comedian and actor. Joey also hosts his own podcast called “The Church of What’s Happening Now”. https://www.youtube.com/channel/UCv695o3i-JmkUB7tPbtwXDA'],\n",
       " 'urls': ['https://podscribe.app/feeds/http-joeroganexpjoeroganlibsynprocom-rss/episodes/7f3d3089c8eb4fa2b02c76b0954d8344',\n",
       "  'https://podscribe.app/feeds/http-joeroganexpjoeroganlibsynprocom-rss/episodes/7798906218684900b35f1eeeaa5c2d64',\n",
       "  'https://podscribe.app/feeds/http-joeroganexpjoeroganlibsynprocom-rss/episodes/a2d1426dd1674a8abe405ea331bfcc46',\n",
       "  'https://podscribe.app/feeds/http-joeroganexpjoeroganlibsynprocom-rss/episodes/79a35d049da64c298190f1d96a2fcb35',\n",
       "  'https://podscribe.app/feeds/http-joeroganexpjoeroganlibsynprocom-rss/episodes/6976d22f286e4ba7b784be39fa3c09a0']}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(a,open('flask/data/recommender.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = []\n",
    "descs = []\n",
    "urls = []\n",
    "for k,v in f.items():\n",
    "    episodes.append(k)\n",
    "    descs.append(v['desc'])\n",
    "    urls.append(v['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {'episodes':episodes, 'descs':descs, 'urls':urls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#1310 Sober October 2019 Preview'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict['episodes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= ['one', 'two','three']\n",
    "b = ['four', 'five', 'six']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpc_list = []\n",
    "for i,v in enumerate(a):\n",
    "    tpc_list.append((a[i],b[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 'four'), ('two', 'five'), ('three', 'six')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpc_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
